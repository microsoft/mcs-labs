<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Measure Success | Microsoft Copilot Studio Labs</title>
    <link rel="stylesheet" href="/mcs-labs/assets/css/style.css">
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Measure Success | Microsoft Copilot Studio Labs</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Measure Success" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Measure success: Track conversation outcomes and user feedback on AI responses You can’t improve what you can’t measure: design your agent to track successful and unsuccessful outcomes while collecting user feedback on AI-generated responses. 🧭 Lab Details Level Persona Duration Purpose 300 Advanced Maker 60 minutes After completing this lab, participants will be able to design an agent that tracks conversation outcomes and collects user feedback on AI-generated responses. They will gain meaningful analytics to identify which knowledge sources drive the highest satisfaction (CSAT) and understand patterns leading to abandoned or escalated conversations. 📚 Table of Contents Why This Matters Introduction Core Concepts Overview Documentation and Additional Training Links Prerequisites Summary of Targets Use Cases Covered Instructions by Use Case Use Case #1: The end of conversation topic Use Case #2: Create a smooth and intuitive end-of-conversation experience 🤔 Why This Matters Advanced Makers - Many organizations struggle with understanding whether their AI agents are truly helping users or creating frustration through abandoned conversations. Think of running a retail store without knowing if customers leave satisfied or empty-handed: Without conversation tracking: You see usage numbers but can’t identify why users abandon conversations or which responses cause dissatisfaction With conversation tracking: You gain actionable insights into user satisfaction, identify problematic knowledge sources, and continuously improve response quality Common challenges solved by this lab: “Our analytics show high usage but we don’t know if users are actually getting help” “Users seem to abandon conversations but we don’t understand why” “We can’t identify which knowledge sources need improvement” “There’s no way to capture user feedback on AI-generated responses” The 60 minutes you invest in this lab will transform your agent from a black box into a data-driven tool for continuous improvement. 🌐 Introduction In today’s AI-driven customer service landscape, simply having an agent that responds isn’t enough. Organizations need to understand whether their AI assistants are genuinely solving user problems or creating barriers to success. Real-world example: A company deployed an AI assistant on their website to handle product inquiries. Initially, they celebrated high engagement metrics—thousands of conversations daily. However, customer satisfaction surveys revealed declining scores, and support tickets actually increased. The problem? Their AI was providing responses that looked helpful but weren’t actually resolving user queries. Without proper conversation outcome tracking and feedback collection, they were flying blind. After implementing the strategies in this lab, they discovered that 40% of conversations were being abandoned after the first AI response, and users were providing consistent feedback about specific knowledge gaps. This insight allowed them to refine their content strategy, resulting in a 65% improvement in conversation resolution rates and significantly higher customer satisfaction scores. 🎓 Core Concepts Overview Concept Why it matters End of Conversation Topic Provides structured tracking of conversation outcomes (resolved, abandoned, escalated) enabling accurate measurement of agent effectiveness and user satisfaction patterns. Conversation Resolution Tracking Distinguishes between implicit and explicit resolution, allowing you to understand not just if users got answers, but whether they were satisfied with those answers. Adaptive Card Feedback Collection Creates unobtrusive, intuitive feedback mechanisms that capture user sentiment without disrupting conversation flow, increasing response rates and data quality. Session-based Analytics Tracks individual user requests within conversations separately, providing granular insights into which types of queries succeed or fail most often. CSAT Integration Connects user satisfaction scores directly to specific knowledge sources and response patterns, enabling data-driven content improvement strategies. Generative AI Behavior Control Ensures proper outcome tracking even when using dynamic AI responses, maintaining visibility into user interactions across different conversation modes. 📄 Documentation and Additional Training Links Measuring agent engagement Measuring agent outcomes Deflection overview Key concepts – Analytics Custom analytics strategy ✅ Prerequisites You need to have access to Microsoft Copilot Studio using https://copilotstudio.microsoft.com/. You can either customize the agent from LAB-10 Create a knowledge agent for your public website or create a new agent with at least one knowledge source. Basic understanding of Copilot Studio topics and conversation flow design. 🎯 Summary of Targets In this lab, you’ll transform your AI agent from a simple question-answering tool into a sophisticated system that tracks success and learns from user feedback. By the end of the lab, you will: Configure conversation outcome tracking using the End of Conversation topic to measure resolved, abandoned, and escalated interactions. Implement intuitive feedback collection through thumbs-up/thumbs-down reactions that don’t disrupt conversation flow. Create detailed feedback capture for negative reactions, allowing users to provide specific improvement suggestions. Design smooth conversation endings that balance user experience with data collection needs. Understand analytics insights to identify which knowledge sources drive highest satisfaction and recognize abandonment patterns. 🧩 Use Cases Covered Step Use Case Value added Effort 1 The end of conversation topic Effectively manage user interactions by understanding when and how to seamlessly redirect users to the end-of-conversation topic, enabling accurate tracking of conversation outcomes. 15 min 2 Create a smooth and intuitive end-of-conversation experience Customize the default end-of-conversation topic to create a more seamless, conversational-friendly experience for users. 15 min 🛠️ Instructions by Use Case 🧱 Use Case #1: The end of conversation topic Every conversation should have a conclusion – Design for clear outcomes. Use case Value added Estimated effort The end of conversation topic Effectively manage user interactions by understanding when and how to seamlessly redirect users to the end-of-conversation topic, enabling accurate tracking of conversation outcomes. 15 minutes Summary of tasks In this section, you’ll learn how the end of conversation topic works in Copilot Studio and how to use it effectively in your conversation design. By structuring conversations for clear outcomes, you’ll enable meaningful analytics and actionable insights to improve your agent’s performance. Scenario: You’ve built an agent with the knowledge to answer user questions—but is it actually delivering? If your analytics dashboard isn’t showing meaningful data, it’s time to track successful conversation outcomes and CSAT scores. You can’t improve what you can’t measure. Objective Configure your agent to properly track conversation outcomes by implementing the End of Conversation topic and understanding how it integrates with both classic and generative orchestration modes. Step-by-step instructions Understanding the End of Conversation Topic Navigate to the Copilot Studio agent you have created for this lab (e.g., LAB-10, or a new one). https://aka.ms/MCSStart Go to the Topics tab, display All, and select End of conversation. Explore what the topic is doing. [!TIP] The end of conversation topic is meant to be triggered when the agent has presumably fulfilled the user’s request. This can happen either after providing a direct answer, such as retrieving information from knowledge sources, or after completing a more complex multi-turn interaction where the user and agent exchange multiple messages to complete a task. By default, when the conversation reaches this stage, the agent asks, “Did this answer your question?” At this point, the resolution is considered implicit, meaning that if the user leaves without responding, it is assumed that their query was resolved. However, if the user confirms that their question was answered, the resolution becomes explicit, and they are then prompted to provide a Customer Satisfaction Score (CSAT) to rate their experience. Configuring Classic Orchestration Mode Explore other topics in your agent. By default, are they redirecting to the End of conversation topic? For newly created agents, that shouldn’t be the case. The End of conversation topic must be redirected to explicitly from the places where you feel the user request has been fulfilled. Assuming the Generative mode for orchestration is Disabled on your agent (you can see that option either in the Overview tab, or in Settings, under Generative AI). Let’s add a redirect to the End of conversation topic from the Conversational boosting topic. Go to the Topics tab, display All, and select Conversational boosting. Delete the End current topic node, and instead, add a new node: Topic management &gt; Go to another topic &gt; and select End of Conversation. Save the topic. Testing Classic Mode Behavior Now, test your agent in the test pane by asking a question that will trigger the Conversational boosting topic. For example, you may ask: What are the key metrics offered by the analytics dashboard? Answer the different questions until you can ask a new question. In the happy path, notice you must answer 3 questions before you can ask a new question. Configuring Generative Mode Let’s now try what the experience is by toggling the Generative mode on. You can enable it either in the Overview tab, or in Settings, under Generative AI. Refresh the test pane, and ask the same question. What are the key metrics offered by the analytics dashboard? Notice how the experience is different. The Activity map is displayed and shows you the agent’s reasoning based on the user query. [!IMPORTANT] In the test pane, notice that you are no longer prompted with “Did this answer your question?”. Why isn’t End of conversation triggered? That is because the Conversational boosting topic wasn’t traversed with generative mode. Creating the Plan Complete Topic Go to the Topics tab, select + Add a topic, and choose From blank. Don’t leave it with the default “Untitled” label. Select Untitled and change the text to Plan complete. Then, change the trigger by hovering over the “Triggered by agent” box until the icon to swap the trigger for another type appears. Then scroll down and choose Plan complete. Add a new node and select Topic management &gt; Go to another topic &gt; End of Conversation. Save the topic. Now, refresh the test pane, and test your agent again. What are the key metrics offered by the analytics dashboard? Notice that after the answer is provided by the agent, the user is prompted for confirmation. Understanding Topic Behavior Refresh the test pane and send a simple, everyday message, like “hello”. Hi! Notice how the End of conversation topic isn’t triggered. Why is that? Open the Greeting topic by clicking the edit (✏️) icon. Notice how the End all topics node prevents this behavior. Any subsequent user messages remain within the same conversation session, as the agent assumes the user’s request hasn’t yet been resolved. For the rest of the lab, you may disable the generative mode. 🏅 Congratulations! You’ve completed Use Case #1! Test your understanding Key takeaways: Conversation resolution tracking – Redirecting to the end of conversation topic allows you to track successful, abandoned, and escalated interactions. Session-based analytics – Conversations can contain multiple sessions, each with a distinct outcome (resolved, escalated, abandoned). Customizing conversation endings – The end of conversation topic can be tailored to enhance user experience, ensuring smooth and meaningful conversation conclusions. Lessons learned &amp; troubleshooting tips: If your analytics dashboard is showing too many abandoned sessions, check if conversations properly redirect to the end of conversation topic. Analytics dashboards don’t show sessions from your own tests in the test pane. Only the interactions that happened over your deployed channels will show. When using conversational boosting, ensure the topic transitions correctly to the end of conversation topic to capture user feedback. Challenge: Apply this to your own use case How can you integrate clear conversation endings into your existing agent topics? Where should you collect user feedback to improve response quality? What patterns can you identify in abandoned vs. resolved conversations? 🔄 Use Case #2: Create a smooth and intuitive end-of-conversation experience End conversations without friction – create a smooth, unobtrusive way to gather feedback without disrupting the flow. Use case Value added Estimated effort Create a smooth and intuitive end-of-conversation experience Customize the default end-of-conversation topic to create a more seamless, conversational-friendly experience for users. 15 minutes Summary of tasks In this section, you’ll learn how the default End of conversation topic can unintentionally interrupt the user’s conversational flow, forcing unnecessary feedback prompts or confirmations. You’ll see how to modify this default behavior to create a smoother and more intuitive experience. Scenario: Visitors on your website frequently have multiple related questions about products and solutions. The default End of conversation prompt can disrupt their experience by forcing them into providing feedback or acknowledgments prematurely. By customizing the End of conversation topic, you’ll enable a more fluid interaction, allowing users to naturally continue conversations without friction or interruption. Step-by-step instructions Let’s start with a test. Refresh the test pane and ask two questions consecutively. What is Copilot Studio? What knowledge sources does it support? Notice how the default End of conversation topic interrupts the interaction, preventing the second question from being answered until the user responds to the prompt, “Did that answer your question?” Go to the Topics tab, display All, and select End of conversation. On the question “Did that answer your question?”, select the ellipsis (…) and open properties. Go to Question behavior. Set How many reprompts to Don’t repeat. Return to the Question Properties, then select Entity recognition: For Action if no entity found, choose Set variable to empty (no value). Below the “Did that answer your question?” question, notice the condition only tests if SurveyResponse is true. Let’s add another condition path, by clicking on the (➕) action above the various conditions. In Select a variable, choose SurveyResponse. For the test, leave it to is equal to, and set false for the value. Move everything that is under All other conditions under the new false branch by cutting and pasting the content. Add a redirect to the Conversational boosting topic under the All other conditions path. Save your topic. Let’s do a new test. Refresh the test pane and ask again two questions consecutively. What is Copilot Studio? What knowledge sources does it support? Notice how follow-up questions are no longer blocking the conversation flow. [!TIP] If you want the user’s follow-up questions to trigger existing topics rather than always defaulting to Conversational boosting, verify the Interruptions setting in the “Did that answer your question?” question properties. By default, they allow interruptions, meaning the agent can seamlessly switch to a recognized topic based on the user’s next input. You can further simplify what happens after the user answers Yes. After the CSAT question, add a message node asking: Thank you for your feedback! Feel free to ask me something else. Then add a node Topic management &gt; End conversation. Delete everything further below that path. You can further simplify what happens after the user answers No. Under the SurveyResponse false condition path, add a new message node: Sorry I wasn&#39;t able to help better. You may try reaching out to our [Microsoft Copilot Studio community](https://aka.ms/CopilotStudioCommunity) or submitting a [support request](https://learn.microsoft.com/en-us/power-platform/admin/get-help-support). Would you like to try again? Feel free to ask a new question. Then add a node Topic management &gt; End conversation. Save topic. 🏅 Congratulations! You’ve completed Use Case #2! Test your understanding How does customizing the End of Conversation topic improve user experience without sacrificing data collection? What are the trade-offs between gathering comprehensive feedback and maintaining conversation flow? How can you apply these principles to create more natural conversation endings in your own agents? Challenge: Apply this to your own use case Where in your current agent can users experience unnecessary conversational friction? How can you apply these customization strategies to ensure smoother conversation endings? In what ways can user feedback be naturally integrated without interrupting conversational flow? 🏆 Summary of learnings True learning comes from doing, questioning, and reflecting—so let’s put your skills to the test. To maximize the impact of conversation outcome tracking: Structure conversations for clear outcomes – Every interaction should have a definitive end point that can be measured and analyzed for continuous improvement. Balance user experience with data collection – Gather meaningful feedback without creating friction that drives users away from your agent. Implement session-based tracking – Understand that individual queries within conversations have separate outcomes that provide granular insights. Leverage both implicit and explicit resolution – Track when users are satisfied enough to leave (implicit) versus when they explicitly confirm satisfaction. Design for different orchestration modes – Ensure your tracking works effectively whether using classic topics or generative AI responses. Conclusions and recommendations Conversation analytics golden rules: Always redirect to the End of Conversation topic when user requests are fulfilled to ensure accurate outcome tracking. Customize feedback prompts to match your users’ communication style and reduce abandonment rates. Test conversation flows regularly to identify where users experience friction or confusion. Use analytics insights to identify knowledge gaps and refine content strategy continuously. Balance comprehensive data collection with user experience to maintain high engagement levels. Implement both lightweight feedback (thumbs up/down) and detailed feedback collection for comprehensive insights. By following these principles, you’ll transform your AI agent from a simple response tool into a data-driven system that continuously learns and improves, delivering measurably better user experiences and business outcomes." />
<meta property="og:description" content="Measure success: Track conversation outcomes and user feedback on AI responses You can’t improve what you can’t measure: design your agent to track successful and unsuccessful outcomes while collecting user feedback on AI-generated responses. 🧭 Lab Details Level Persona Duration Purpose 300 Advanced Maker 60 minutes After completing this lab, participants will be able to design an agent that tracks conversation outcomes and collects user feedback on AI-generated responses. They will gain meaningful analytics to identify which knowledge sources drive the highest satisfaction (CSAT) and understand patterns leading to abandoned or escalated conversations. 📚 Table of Contents Why This Matters Introduction Core Concepts Overview Documentation and Additional Training Links Prerequisites Summary of Targets Use Cases Covered Instructions by Use Case Use Case #1: The end of conversation topic Use Case #2: Create a smooth and intuitive end-of-conversation experience 🤔 Why This Matters Advanced Makers - Many organizations struggle with understanding whether their AI agents are truly helping users or creating frustration through abandoned conversations. Think of running a retail store without knowing if customers leave satisfied or empty-handed: Without conversation tracking: You see usage numbers but can’t identify why users abandon conversations or which responses cause dissatisfaction With conversation tracking: You gain actionable insights into user satisfaction, identify problematic knowledge sources, and continuously improve response quality Common challenges solved by this lab: “Our analytics show high usage but we don’t know if users are actually getting help” “Users seem to abandon conversations but we don’t understand why” “We can’t identify which knowledge sources need improvement” “There’s no way to capture user feedback on AI-generated responses” The 60 minutes you invest in this lab will transform your agent from a black box into a data-driven tool for continuous improvement. 🌐 Introduction In today’s AI-driven customer service landscape, simply having an agent that responds isn’t enough. Organizations need to understand whether their AI assistants are genuinely solving user problems or creating barriers to success. Real-world example: A company deployed an AI assistant on their website to handle product inquiries. Initially, they celebrated high engagement metrics—thousands of conversations daily. However, customer satisfaction surveys revealed declining scores, and support tickets actually increased. The problem? Their AI was providing responses that looked helpful but weren’t actually resolving user queries. Without proper conversation outcome tracking and feedback collection, they were flying blind. After implementing the strategies in this lab, they discovered that 40% of conversations were being abandoned after the first AI response, and users were providing consistent feedback about specific knowledge gaps. This insight allowed them to refine their content strategy, resulting in a 65% improvement in conversation resolution rates and significantly higher customer satisfaction scores. 🎓 Core Concepts Overview Concept Why it matters End of Conversation Topic Provides structured tracking of conversation outcomes (resolved, abandoned, escalated) enabling accurate measurement of agent effectiveness and user satisfaction patterns. Conversation Resolution Tracking Distinguishes between implicit and explicit resolution, allowing you to understand not just if users got answers, but whether they were satisfied with those answers. Adaptive Card Feedback Collection Creates unobtrusive, intuitive feedback mechanisms that capture user sentiment without disrupting conversation flow, increasing response rates and data quality. Session-based Analytics Tracks individual user requests within conversations separately, providing granular insights into which types of queries succeed or fail most often. CSAT Integration Connects user satisfaction scores directly to specific knowledge sources and response patterns, enabling data-driven content improvement strategies. Generative AI Behavior Control Ensures proper outcome tracking even when using dynamic AI responses, maintaining visibility into user interactions across different conversation modes. 📄 Documentation and Additional Training Links Measuring agent engagement Measuring agent outcomes Deflection overview Key concepts – Analytics Custom analytics strategy ✅ Prerequisites You need to have access to Microsoft Copilot Studio using https://copilotstudio.microsoft.com/. You can either customize the agent from LAB-10 Create a knowledge agent for your public website or create a new agent with at least one knowledge source. Basic understanding of Copilot Studio topics and conversation flow design. 🎯 Summary of Targets In this lab, you’ll transform your AI agent from a simple question-answering tool into a sophisticated system that tracks success and learns from user feedback. By the end of the lab, you will: Configure conversation outcome tracking using the End of Conversation topic to measure resolved, abandoned, and escalated interactions. Implement intuitive feedback collection through thumbs-up/thumbs-down reactions that don’t disrupt conversation flow. Create detailed feedback capture for negative reactions, allowing users to provide specific improvement suggestions. Design smooth conversation endings that balance user experience with data collection needs. Understand analytics insights to identify which knowledge sources drive highest satisfaction and recognize abandonment patterns. 🧩 Use Cases Covered Step Use Case Value added Effort 1 The end of conversation topic Effectively manage user interactions by understanding when and how to seamlessly redirect users to the end-of-conversation topic, enabling accurate tracking of conversation outcomes. 15 min 2 Create a smooth and intuitive end-of-conversation experience Customize the default end-of-conversation topic to create a more seamless, conversational-friendly experience for users. 15 min 🛠️ Instructions by Use Case 🧱 Use Case #1: The end of conversation topic Every conversation should have a conclusion – Design for clear outcomes. Use case Value added Estimated effort The end of conversation topic Effectively manage user interactions by understanding when and how to seamlessly redirect users to the end-of-conversation topic, enabling accurate tracking of conversation outcomes. 15 minutes Summary of tasks In this section, you’ll learn how the end of conversation topic works in Copilot Studio and how to use it effectively in your conversation design. By structuring conversations for clear outcomes, you’ll enable meaningful analytics and actionable insights to improve your agent’s performance. Scenario: You’ve built an agent with the knowledge to answer user questions—but is it actually delivering? If your analytics dashboard isn’t showing meaningful data, it’s time to track successful conversation outcomes and CSAT scores. You can’t improve what you can’t measure. Objective Configure your agent to properly track conversation outcomes by implementing the End of Conversation topic and understanding how it integrates with both classic and generative orchestration modes. Step-by-step instructions Understanding the End of Conversation Topic Navigate to the Copilot Studio agent you have created for this lab (e.g., LAB-10, or a new one). https://aka.ms/MCSStart Go to the Topics tab, display All, and select End of conversation. Explore what the topic is doing. [!TIP] The end of conversation topic is meant to be triggered when the agent has presumably fulfilled the user’s request. This can happen either after providing a direct answer, such as retrieving information from knowledge sources, or after completing a more complex multi-turn interaction where the user and agent exchange multiple messages to complete a task. By default, when the conversation reaches this stage, the agent asks, “Did this answer your question?” At this point, the resolution is considered implicit, meaning that if the user leaves without responding, it is assumed that their query was resolved. However, if the user confirms that their question was answered, the resolution becomes explicit, and they are then prompted to provide a Customer Satisfaction Score (CSAT) to rate their experience. Configuring Classic Orchestration Mode Explore other topics in your agent. By default, are they redirecting to the End of conversation topic? For newly created agents, that shouldn’t be the case. The End of conversation topic must be redirected to explicitly from the places where you feel the user request has been fulfilled. Assuming the Generative mode for orchestration is Disabled on your agent (you can see that option either in the Overview tab, or in Settings, under Generative AI). Let’s add a redirect to the End of conversation topic from the Conversational boosting topic. Go to the Topics tab, display All, and select Conversational boosting. Delete the End current topic node, and instead, add a new node: Topic management &gt; Go to another topic &gt; and select End of Conversation. Save the topic. Testing Classic Mode Behavior Now, test your agent in the test pane by asking a question that will trigger the Conversational boosting topic. For example, you may ask: What are the key metrics offered by the analytics dashboard? Answer the different questions until you can ask a new question. In the happy path, notice you must answer 3 questions before you can ask a new question. Configuring Generative Mode Let’s now try what the experience is by toggling the Generative mode on. You can enable it either in the Overview tab, or in Settings, under Generative AI. Refresh the test pane, and ask the same question. What are the key metrics offered by the analytics dashboard? Notice how the experience is different. The Activity map is displayed and shows you the agent’s reasoning based on the user query. [!IMPORTANT] In the test pane, notice that you are no longer prompted with “Did this answer your question?”. Why isn’t End of conversation triggered? That is because the Conversational boosting topic wasn’t traversed with generative mode. Creating the Plan Complete Topic Go to the Topics tab, select + Add a topic, and choose From blank. Don’t leave it with the default “Untitled” label. Select Untitled and change the text to Plan complete. Then, change the trigger by hovering over the “Triggered by agent” box until the icon to swap the trigger for another type appears. Then scroll down and choose Plan complete. Add a new node and select Topic management &gt; Go to another topic &gt; End of Conversation. Save the topic. Now, refresh the test pane, and test your agent again. What are the key metrics offered by the analytics dashboard? Notice that after the answer is provided by the agent, the user is prompted for confirmation. Understanding Topic Behavior Refresh the test pane and send a simple, everyday message, like “hello”. Hi! Notice how the End of conversation topic isn’t triggered. Why is that? Open the Greeting topic by clicking the edit (✏️) icon. Notice how the End all topics node prevents this behavior. Any subsequent user messages remain within the same conversation session, as the agent assumes the user’s request hasn’t yet been resolved. For the rest of the lab, you may disable the generative mode. 🏅 Congratulations! You’ve completed Use Case #1! Test your understanding Key takeaways: Conversation resolution tracking – Redirecting to the end of conversation topic allows you to track successful, abandoned, and escalated interactions. Session-based analytics – Conversations can contain multiple sessions, each with a distinct outcome (resolved, escalated, abandoned). Customizing conversation endings – The end of conversation topic can be tailored to enhance user experience, ensuring smooth and meaningful conversation conclusions. Lessons learned &amp; troubleshooting tips: If your analytics dashboard is showing too many abandoned sessions, check if conversations properly redirect to the end of conversation topic. Analytics dashboards don’t show sessions from your own tests in the test pane. Only the interactions that happened over your deployed channels will show. When using conversational boosting, ensure the topic transitions correctly to the end of conversation topic to capture user feedback. Challenge: Apply this to your own use case How can you integrate clear conversation endings into your existing agent topics? Where should you collect user feedback to improve response quality? What patterns can you identify in abandoned vs. resolved conversations? 🔄 Use Case #2: Create a smooth and intuitive end-of-conversation experience End conversations without friction – create a smooth, unobtrusive way to gather feedback without disrupting the flow. Use case Value added Estimated effort Create a smooth and intuitive end-of-conversation experience Customize the default end-of-conversation topic to create a more seamless, conversational-friendly experience for users. 15 minutes Summary of tasks In this section, you’ll learn how the default End of conversation topic can unintentionally interrupt the user’s conversational flow, forcing unnecessary feedback prompts or confirmations. You’ll see how to modify this default behavior to create a smoother and more intuitive experience. Scenario: Visitors on your website frequently have multiple related questions about products and solutions. The default End of conversation prompt can disrupt their experience by forcing them into providing feedback or acknowledgments prematurely. By customizing the End of conversation topic, you’ll enable a more fluid interaction, allowing users to naturally continue conversations without friction or interruption. Step-by-step instructions Let’s start with a test. Refresh the test pane and ask two questions consecutively. What is Copilot Studio? What knowledge sources does it support? Notice how the default End of conversation topic interrupts the interaction, preventing the second question from being answered until the user responds to the prompt, “Did that answer your question?” Go to the Topics tab, display All, and select End of conversation. On the question “Did that answer your question?”, select the ellipsis (…) and open properties. Go to Question behavior. Set How many reprompts to Don’t repeat. Return to the Question Properties, then select Entity recognition: For Action if no entity found, choose Set variable to empty (no value). Below the “Did that answer your question?” question, notice the condition only tests if SurveyResponse is true. Let’s add another condition path, by clicking on the (➕) action above the various conditions. In Select a variable, choose SurveyResponse. For the test, leave it to is equal to, and set false for the value. Move everything that is under All other conditions under the new false branch by cutting and pasting the content. Add a redirect to the Conversational boosting topic under the All other conditions path. Save your topic. Let’s do a new test. Refresh the test pane and ask again two questions consecutively. What is Copilot Studio? What knowledge sources does it support? Notice how follow-up questions are no longer blocking the conversation flow. [!TIP] If you want the user’s follow-up questions to trigger existing topics rather than always defaulting to Conversational boosting, verify the Interruptions setting in the “Did that answer your question?” question properties. By default, they allow interruptions, meaning the agent can seamlessly switch to a recognized topic based on the user’s next input. You can further simplify what happens after the user answers Yes. After the CSAT question, add a message node asking: Thank you for your feedback! Feel free to ask me something else. Then add a node Topic management &gt; End conversation. Delete everything further below that path. You can further simplify what happens after the user answers No. Under the SurveyResponse false condition path, add a new message node: Sorry I wasn&#39;t able to help better. You may try reaching out to our [Microsoft Copilot Studio community](https://aka.ms/CopilotStudioCommunity) or submitting a [support request](https://learn.microsoft.com/en-us/power-platform/admin/get-help-support). Would you like to try again? Feel free to ask a new question. Then add a node Topic management &gt; End conversation. Save topic. 🏅 Congratulations! You’ve completed Use Case #2! Test your understanding How does customizing the End of Conversation topic improve user experience without sacrificing data collection? What are the trade-offs between gathering comprehensive feedback and maintaining conversation flow? How can you apply these principles to create more natural conversation endings in your own agents? Challenge: Apply this to your own use case Where in your current agent can users experience unnecessary conversational friction? How can you apply these customization strategies to ensure smoother conversation endings? In what ways can user feedback be naturally integrated without interrupting conversational flow? 🏆 Summary of learnings True learning comes from doing, questioning, and reflecting—so let’s put your skills to the test. To maximize the impact of conversation outcome tracking: Structure conversations for clear outcomes – Every interaction should have a definitive end point that can be measured and analyzed for continuous improvement. Balance user experience with data collection – Gather meaningful feedback without creating friction that drives users away from your agent. Implement session-based tracking – Understand that individual queries within conversations have separate outcomes that provide granular insights. Leverage both implicit and explicit resolution – Track when users are satisfied enough to leave (implicit) versus when they explicitly confirm satisfaction. Design for different orchestration modes – Ensure your tracking works effectively whether using classic topics or generative AI responses. Conclusions and recommendations Conversation analytics golden rules: Always redirect to the End of Conversation topic when user requests are fulfilled to ensure accurate outcome tracking. Customize feedback prompts to match your users’ communication style and reduce abandonment rates. Test conversation flows regularly to identify where users experience friction or confusion. Use analytics insights to identify knowledge gaps and refine content strategy continuously. Balance comprehensive data collection with user experience to maintain high engagement levels. Implement both lightweight feedback (thumbs up/down) and detailed feedback collection for comprehensive insights. By following these principles, you’ll transform your AI agent from a simple response tool into a data-driven system that continuously learns and improves, delivering measurably better user experiences and business outcomes." />
<link rel="canonical" href="http://0.0.0.0:4000/mcs-labs/labs/measure-success/" />
<meta property="og:url" content="http://0.0.0.0:4000/mcs-labs/labs/measure-success/" />
<meta property="og:site_name" content="Microsoft Copilot Studio Labs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-10-08T15:03:33-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Measure Success" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-10-08T15:03:33-05:00","datePublished":"2025-10-08T15:03:33-05:00","description":"Measure success: Track conversation outcomes and user feedback on AI responses You can’t improve what you can’t measure: design your agent to track successful and unsuccessful outcomes while collecting user feedback on AI-generated responses. 🧭 Lab Details Level Persona Duration Purpose 300 Advanced Maker 60 minutes After completing this lab, participants will be able to design an agent that tracks conversation outcomes and collects user feedback on AI-generated responses. They will gain meaningful analytics to identify which knowledge sources drive the highest satisfaction (CSAT) and understand patterns leading to abandoned or escalated conversations. 📚 Table of Contents Why This Matters Introduction Core Concepts Overview Documentation and Additional Training Links Prerequisites Summary of Targets Use Cases Covered Instructions by Use Case Use Case #1: The end of conversation topic Use Case #2: Create a smooth and intuitive end-of-conversation experience 🤔 Why This Matters Advanced Makers - Many organizations struggle with understanding whether their AI agents are truly helping users or creating frustration through abandoned conversations. Think of running a retail store without knowing if customers leave satisfied or empty-handed: Without conversation tracking: You see usage numbers but can’t identify why users abandon conversations or which responses cause dissatisfaction With conversation tracking: You gain actionable insights into user satisfaction, identify problematic knowledge sources, and continuously improve response quality Common challenges solved by this lab: “Our analytics show high usage but we don’t know if users are actually getting help” “Users seem to abandon conversations but we don’t understand why” “We can’t identify which knowledge sources need improvement” “There’s no way to capture user feedback on AI-generated responses” The 60 minutes you invest in this lab will transform your agent from a black box into a data-driven tool for continuous improvement. 🌐 Introduction In today’s AI-driven customer service landscape, simply having an agent that responds isn’t enough. Organizations need to understand whether their AI assistants are genuinely solving user problems or creating barriers to success. Real-world example: A company deployed an AI assistant on their website to handle product inquiries. Initially, they celebrated high engagement metrics—thousands of conversations daily. However, customer satisfaction surveys revealed declining scores, and support tickets actually increased. The problem? Their AI was providing responses that looked helpful but weren’t actually resolving user queries. Without proper conversation outcome tracking and feedback collection, they were flying blind. After implementing the strategies in this lab, they discovered that 40% of conversations were being abandoned after the first AI response, and users were providing consistent feedback about specific knowledge gaps. This insight allowed them to refine their content strategy, resulting in a 65% improvement in conversation resolution rates and significantly higher customer satisfaction scores. 🎓 Core Concepts Overview Concept Why it matters End of Conversation Topic Provides structured tracking of conversation outcomes (resolved, abandoned, escalated) enabling accurate measurement of agent effectiveness and user satisfaction patterns. Conversation Resolution Tracking Distinguishes between implicit and explicit resolution, allowing you to understand not just if users got answers, but whether they were satisfied with those answers. Adaptive Card Feedback Collection Creates unobtrusive, intuitive feedback mechanisms that capture user sentiment without disrupting conversation flow, increasing response rates and data quality. Session-based Analytics Tracks individual user requests within conversations separately, providing granular insights into which types of queries succeed or fail most often. CSAT Integration Connects user satisfaction scores directly to specific knowledge sources and response patterns, enabling data-driven content improvement strategies. Generative AI Behavior Control Ensures proper outcome tracking even when using dynamic AI responses, maintaining visibility into user interactions across different conversation modes. 📄 Documentation and Additional Training Links Measuring agent engagement Measuring agent outcomes Deflection overview Key concepts – Analytics Custom analytics strategy ✅ Prerequisites You need to have access to Microsoft Copilot Studio using https://copilotstudio.microsoft.com/. You can either customize the agent from LAB-10 Create a knowledge agent for your public website or create a new agent with at least one knowledge source. Basic understanding of Copilot Studio topics and conversation flow design. 🎯 Summary of Targets In this lab, you’ll transform your AI agent from a simple question-answering tool into a sophisticated system that tracks success and learns from user feedback. By the end of the lab, you will: Configure conversation outcome tracking using the End of Conversation topic to measure resolved, abandoned, and escalated interactions. Implement intuitive feedback collection through thumbs-up/thumbs-down reactions that don’t disrupt conversation flow. Create detailed feedback capture for negative reactions, allowing users to provide specific improvement suggestions. Design smooth conversation endings that balance user experience with data collection needs. Understand analytics insights to identify which knowledge sources drive highest satisfaction and recognize abandonment patterns. 🧩 Use Cases Covered Step Use Case Value added Effort 1 The end of conversation topic Effectively manage user interactions by understanding when and how to seamlessly redirect users to the end-of-conversation topic, enabling accurate tracking of conversation outcomes. 15 min 2 Create a smooth and intuitive end-of-conversation experience Customize the default end-of-conversation topic to create a more seamless, conversational-friendly experience for users. 15 min 🛠️ Instructions by Use Case 🧱 Use Case #1: The end of conversation topic Every conversation should have a conclusion – Design for clear outcomes. Use case Value added Estimated effort The end of conversation topic Effectively manage user interactions by understanding when and how to seamlessly redirect users to the end-of-conversation topic, enabling accurate tracking of conversation outcomes. 15 minutes Summary of tasks In this section, you’ll learn how the end of conversation topic works in Copilot Studio and how to use it effectively in your conversation design. By structuring conversations for clear outcomes, you’ll enable meaningful analytics and actionable insights to improve your agent’s performance. Scenario: You’ve built an agent with the knowledge to answer user questions—but is it actually delivering? If your analytics dashboard isn’t showing meaningful data, it’s time to track successful conversation outcomes and CSAT scores. You can’t improve what you can’t measure. Objective Configure your agent to properly track conversation outcomes by implementing the End of Conversation topic and understanding how it integrates with both classic and generative orchestration modes. Step-by-step instructions Understanding the End of Conversation Topic Navigate to the Copilot Studio agent you have created for this lab (e.g., LAB-10, or a new one). https://aka.ms/MCSStart Go to the Topics tab, display All, and select End of conversation. Explore what the topic is doing. [!TIP] The end of conversation topic is meant to be triggered when the agent has presumably fulfilled the user’s request. This can happen either after providing a direct answer, such as retrieving information from knowledge sources, or after completing a more complex multi-turn interaction where the user and agent exchange multiple messages to complete a task. By default, when the conversation reaches this stage, the agent asks, “Did this answer your question?” At this point, the resolution is considered implicit, meaning that if the user leaves without responding, it is assumed that their query was resolved. However, if the user confirms that their question was answered, the resolution becomes explicit, and they are then prompted to provide a Customer Satisfaction Score (CSAT) to rate their experience. Configuring Classic Orchestration Mode Explore other topics in your agent. By default, are they redirecting to the End of conversation topic? For newly created agents, that shouldn’t be the case. The End of conversation topic must be redirected to explicitly from the places where you feel the user request has been fulfilled. Assuming the Generative mode for orchestration is Disabled on your agent (you can see that option either in the Overview tab, or in Settings, under Generative AI). Let’s add a redirect to the End of conversation topic from the Conversational boosting topic. Go to the Topics tab, display All, and select Conversational boosting. Delete the End current topic node, and instead, add a new node: Topic management &gt; Go to another topic &gt; and select End of Conversation. Save the topic. Testing Classic Mode Behavior Now, test your agent in the test pane by asking a question that will trigger the Conversational boosting topic. For example, you may ask: What are the key metrics offered by the analytics dashboard? Answer the different questions until you can ask a new question. In the happy path, notice you must answer 3 questions before you can ask a new question. Configuring Generative Mode Let’s now try what the experience is by toggling the Generative mode on. You can enable it either in the Overview tab, or in Settings, under Generative AI. Refresh the test pane, and ask the same question. What are the key metrics offered by the analytics dashboard? Notice how the experience is different. The Activity map is displayed and shows you the agent’s reasoning based on the user query. [!IMPORTANT] In the test pane, notice that you are no longer prompted with “Did this answer your question?”. Why isn’t End of conversation triggered? That is because the Conversational boosting topic wasn’t traversed with generative mode. Creating the Plan Complete Topic Go to the Topics tab, select + Add a topic, and choose From blank. Don’t leave it with the default “Untitled” label. Select Untitled and change the text to Plan complete. Then, change the trigger by hovering over the “Triggered by agent” box until the icon to swap the trigger for another type appears. Then scroll down and choose Plan complete. Add a new node and select Topic management &gt; Go to another topic &gt; End of Conversation. Save the topic. Now, refresh the test pane, and test your agent again. What are the key metrics offered by the analytics dashboard? Notice that after the answer is provided by the agent, the user is prompted for confirmation. Understanding Topic Behavior Refresh the test pane and send a simple, everyday message, like “hello”. Hi! Notice how the End of conversation topic isn’t triggered. Why is that? Open the Greeting topic by clicking the edit (✏️) icon. Notice how the End all topics node prevents this behavior. Any subsequent user messages remain within the same conversation session, as the agent assumes the user’s request hasn’t yet been resolved. For the rest of the lab, you may disable the generative mode. 🏅 Congratulations! You’ve completed Use Case #1! Test your understanding Key takeaways: Conversation resolution tracking – Redirecting to the end of conversation topic allows you to track successful, abandoned, and escalated interactions. Session-based analytics – Conversations can contain multiple sessions, each with a distinct outcome (resolved, escalated, abandoned). Customizing conversation endings – The end of conversation topic can be tailored to enhance user experience, ensuring smooth and meaningful conversation conclusions. Lessons learned &amp; troubleshooting tips: If your analytics dashboard is showing too many abandoned sessions, check if conversations properly redirect to the end of conversation topic. Analytics dashboards don’t show sessions from your own tests in the test pane. Only the interactions that happened over your deployed channels will show. When using conversational boosting, ensure the topic transitions correctly to the end of conversation topic to capture user feedback. Challenge: Apply this to your own use case How can you integrate clear conversation endings into your existing agent topics? Where should you collect user feedback to improve response quality? What patterns can you identify in abandoned vs. resolved conversations? 🔄 Use Case #2: Create a smooth and intuitive end-of-conversation experience End conversations without friction – create a smooth, unobtrusive way to gather feedback without disrupting the flow. Use case Value added Estimated effort Create a smooth and intuitive end-of-conversation experience Customize the default end-of-conversation topic to create a more seamless, conversational-friendly experience for users. 15 minutes Summary of tasks In this section, you’ll learn how the default End of conversation topic can unintentionally interrupt the user’s conversational flow, forcing unnecessary feedback prompts or confirmations. You’ll see how to modify this default behavior to create a smoother and more intuitive experience. Scenario: Visitors on your website frequently have multiple related questions about products and solutions. The default End of conversation prompt can disrupt their experience by forcing them into providing feedback or acknowledgments prematurely. By customizing the End of conversation topic, you’ll enable a more fluid interaction, allowing users to naturally continue conversations without friction or interruption. Step-by-step instructions Let’s start with a test. Refresh the test pane and ask two questions consecutively. What is Copilot Studio? What knowledge sources does it support? Notice how the default End of conversation topic interrupts the interaction, preventing the second question from being answered until the user responds to the prompt, “Did that answer your question?” Go to the Topics tab, display All, and select End of conversation. On the question “Did that answer your question?”, select the ellipsis (…) and open properties. Go to Question behavior. Set How many reprompts to Don’t repeat. Return to the Question Properties, then select Entity recognition: For Action if no entity found, choose Set variable to empty (no value). Below the “Did that answer your question?” question, notice the condition only tests if SurveyResponse is true. Let’s add another condition path, by clicking on the (➕) action above the various conditions. In Select a variable, choose SurveyResponse. For the test, leave it to is equal to, and set false for the value. Move everything that is under All other conditions under the new false branch by cutting and pasting the content. Add a redirect to the Conversational boosting topic under the All other conditions path. Save your topic. Let’s do a new test. Refresh the test pane and ask again two questions consecutively. What is Copilot Studio? What knowledge sources does it support? Notice how follow-up questions are no longer blocking the conversation flow. [!TIP] If you want the user’s follow-up questions to trigger existing topics rather than always defaulting to Conversational boosting, verify the Interruptions setting in the “Did that answer your question?” question properties. By default, they allow interruptions, meaning the agent can seamlessly switch to a recognized topic based on the user’s next input. You can further simplify what happens after the user answers Yes. After the CSAT question, add a message node asking: Thank you for your feedback! Feel free to ask me something else. Then add a node Topic management &gt; End conversation. Delete everything further below that path. You can further simplify what happens after the user answers No. Under the SurveyResponse false condition path, add a new message node: Sorry I wasn&#39;t able to help better. You may try reaching out to our [Microsoft Copilot Studio community](https://aka.ms/CopilotStudioCommunity) or submitting a [support request](https://learn.microsoft.com/en-us/power-platform/admin/get-help-support). Would you like to try again? Feel free to ask a new question. Then add a node Topic management &gt; End conversation. Save topic. 🏅 Congratulations! You’ve completed Use Case #2! Test your understanding How does customizing the End of Conversation topic improve user experience without sacrificing data collection? What are the trade-offs between gathering comprehensive feedback and maintaining conversation flow? How can you apply these principles to create more natural conversation endings in your own agents? Challenge: Apply this to your own use case Where in your current agent can users experience unnecessary conversational friction? How can you apply these customization strategies to ensure smoother conversation endings? In what ways can user feedback be naturally integrated without interrupting conversational flow? 🏆 Summary of learnings True learning comes from doing, questioning, and reflecting—so let’s put your skills to the test. To maximize the impact of conversation outcome tracking: Structure conversations for clear outcomes – Every interaction should have a definitive end point that can be measured and analyzed for continuous improvement. Balance user experience with data collection – Gather meaningful feedback without creating friction that drives users away from your agent. Implement session-based tracking – Understand that individual queries within conversations have separate outcomes that provide granular insights. Leverage both implicit and explicit resolution – Track when users are satisfied enough to leave (implicit) versus when they explicitly confirm satisfaction. Design for different orchestration modes – Ensure your tracking works effectively whether using classic topics or generative AI responses. Conclusions and recommendations Conversation analytics golden rules: Always redirect to the End of Conversation topic when user requests are fulfilled to ensure accurate outcome tracking. Customize feedback prompts to match your users’ communication style and reduce abandonment rates. Test conversation flows regularly to identify where users experience friction or confusion. Use analytics insights to identify knowledge gaps and refine content strategy continuously. Balance comprehensive data collection with user experience to maintain high engagement levels. Implement both lightweight feedback (thumbs up/down) and detailed feedback collection for comprehensive insights. By following these principles, you’ll transform your AI agent from a simple response tool into a data-driven system that continuously learns and improves, delivering measurably better user experiences and business outcomes.","headline":"Measure Success","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/mcs-labs/labs/measure-success/"},"url":"http://0.0.0.0:4000/mcs-labs/labs/measure-success/"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>
    <header class="site-header">
        <div class="container">
            <h1><a href="/mcs-labs/">Microsoft Copilot Studio Labs</a></h1>
            <nav>
                <a href="/mcs-labs/">Home</a>
                <a href="/mcs-labs/labs/">All Labs</a>
            </nav>
        </div>
    </header>

    <main class="site-content">
        <div class="container">
            <article class="lab-content">
    <header class="lab-header">
        <h1>Measure Success</h1>
        
        <div class="lab-meta">
            <span class="duration">⏱️ 45 minutes</span>
            
            <span class="difficulty">📊 300</span>
            
        </div>
        
    </header>

    <div class="lab-body">
        <h1 id="measure-success-track-conversation-outcomes-and-user-feedback-on-ai-responses">Measure success: Track conversation outcomes and user feedback on AI responses</h1>

<p>You can’t improve what you can’t measure: design your agent to track successful and unsuccessful outcomes while collecting user feedback on AI-generated responses.</p>

<hr />

<h2 id="-lab-details">🧭 Lab Details</h2>

<table>
  <thead>
    <tr>
      <th>Level</th>
      <th>Persona</th>
      <th>Duration</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>300</td>
      <td>Advanced Maker</td>
      <td>60 minutes</td>
      <td>After completing this lab, participants will be able to design an agent that tracks conversation outcomes and collects user feedback on AI-generated responses. They will gain meaningful analytics to identify which knowledge sources drive the highest satisfaction (CSAT) and understand patterns leading to abandoned or escalated conversations.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="-table-of-contents">📚 Table of Contents</h2>

<ul>
  <li><a href="#-why-this-matters">Why This Matters</a></li>
  <li><a href="#-introduction">Introduction</a></li>
  <li><a href="#-core-concepts-overview">Core Concepts Overview</a></li>
  <li><a href="#-documentation-and-additional-training-links">Documentation and Additional Training Links</a></li>
  <li><a href="#-prerequisites">Prerequisites</a></li>
  <li><a href="#-summary-of-targets">Summary of Targets</a></li>
  <li><a href="#-use-cases-covered">Use Cases Covered</a></li>
  <li><a href="#️-instructions-by-use-case">Instructions by Use Case</a>
    <ul>
      <li><a href="#-use-case-1-the-end-of-conversation-topic">Use Case #1: The end of conversation topic</a></li>
      <li><a href="#-use-case-2-create-a-smooth-and-intuitive-end-of-conversation-experience">Use Case #2: Create a smooth and intuitive end-of-conversation experience</a></li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="-why-this-matters">🤔 Why This Matters</h2>

<p><strong>Advanced Makers</strong> - Many organizations struggle with understanding whether their AI agents are truly helping users or creating frustration through abandoned conversations.</p>

<p>Think of running a retail store without knowing if customers leave satisfied or empty-handed:</p>
<ul>
  <li><strong>Without conversation tracking</strong>: You see usage numbers but can’t identify why users abandon conversations or which responses cause dissatisfaction</li>
  <li><strong>With conversation tracking</strong>: You gain actionable insights into user satisfaction, identify problematic knowledge sources, and continuously improve response quality</li>
</ul>

<p><strong>Common challenges solved by this lab:</strong></p>
<ul>
  <li>“Our analytics show high usage but we don’t know if users are actually getting help”</li>
  <li>“Users seem to abandon conversations but we don’t understand why”</li>
  <li>“We can’t identify which knowledge sources need improvement”</li>
  <li>“There’s no way to capture user feedback on AI-generated responses”</li>
</ul>

<p><strong>The 60 minutes you invest in this lab will transform your agent from a black box into a data-driven tool for continuous improvement.</strong></p>

<hr />

<h2 id="-introduction">🌐 Introduction</h2>

<p>In today’s AI-driven customer service landscape, simply having an agent that responds isn’t enough. Organizations need to understand whether their AI assistants are genuinely solving user problems or creating barriers to success.</p>

<p><strong>Real-world example:</strong> A company deployed an AI assistant on their website to handle product inquiries. Initially, they celebrated high engagement metrics—thousands of conversations daily. However, customer satisfaction surveys revealed declining scores, and support tickets actually increased. The problem? Their AI was providing responses that looked helpful but weren’t actually resolving user queries. Without proper conversation outcome tracking and feedback collection, they were flying blind.</p>

<p>After implementing the strategies in this lab, they discovered that 40% of conversations were being abandoned after the first AI response, and users were providing consistent feedback about specific knowledge gaps. This insight allowed them to refine their content strategy, resulting in a 65% improvement in conversation resolution rates and significantly higher customer satisfaction scores.</p>

<hr />

<h2 id="-core-concepts-overview">🎓 Core Concepts Overview</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>Why it matters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>End of Conversation Topic</strong></td>
      <td>Provides structured tracking of conversation outcomes (resolved, abandoned, escalated) enabling accurate measurement of agent effectiveness and user satisfaction patterns.</td>
    </tr>
    <tr>
      <td><strong>Conversation Resolution Tracking</strong></td>
      <td>Distinguishes between implicit and explicit resolution, allowing you to understand not just if users got answers, but whether they were satisfied with those answers.</td>
    </tr>
    <tr>
      <td><strong>Adaptive Card Feedback Collection</strong></td>
      <td>Creates unobtrusive, intuitive feedback mechanisms that capture user sentiment without disrupting conversation flow, increasing response rates and data quality.</td>
    </tr>
    <tr>
      <td><strong>Session-based Analytics</strong></td>
      <td>Tracks individual user requests within conversations separately, providing granular insights into which types of queries succeed or fail most often.</td>
    </tr>
    <tr>
      <td><strong>CSAT Integration</strong></td>
      <td>Connects user satisfaction scores directly to specific knowledge sources and response patterns, enabling data-driven content improvement strategies.</td>
    </tr>
    <tr>
      <td><strong>Generative AI Behavior Control</strong></td>
      <td>Ensures proper outcome tracking even when using dynamic AI responses, maintaining visibility into user interactions across different conversation modes.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="-documentation-and-additional-training-links">📄 Documentation and Additional Training Links</h2>

<ul>
  <li><a href="https://learn.microsoft.com/en-us/microsoft-copilot-studio/analytics-engagement">Measuring agent engagement</a></li>
  <li><a href="https://learn.microsoft.com/en-us/microsoft-copilot-studio/analytics-outcomes">Measuring agent outcomes</a></li>
  <li><a href="https://learn.microsoft.com/en-us/microsoft-copilot-studio/analytics-deflection">Deflection overview</a></li>
  <li><a href="https://learn.microsoft.com/en-us/microsoft-copilot-studio/analytics-overview">Key concepts – Analytics</a></li>
  <li><a href="https://learn.microsoft.com/en-us/microsoft-copilot-studio/advanced-analytics">Custom analytics strategy</a></li>
</ul>

<hr />

<h2 id="-prerequisites">✅ Prerequisites</h2>

<ul>
  <li>You need to have access to Microsoft Copilot Studio using https://copilotstudio.microsoft.com/.</li>
  <li>You can either customize the agent from LAB-10 Create a knowledge agent for your public website or create a new agent with at least one knowledge source.</li>
  <li>Basic understanding of Copilot Studio topics and conversation flow design.</li>
</ul>

<hr />

<h2 id="-summary-of-targets">🎯 Summary of Targets</h2>

<p>In this lab, you’ll transform your AI agent from a simple question-answering tool into a sophisticated system that tracks success and learns from user feedback. By the end of the lab, you will:</p>

<ul>
  <li><strong>Configure conversation outcome tracking</strong> using the End of Conversation topic to measure resolved, abandoned, and escalated interactions.</li>
  <li><strong>Implement intuitive feedback collection</strong> through thumbs-up/thumbs-down reactions that don’t disrupt conversation flow.</li>
  <li><strong>Create detailed feedback capture</strong> for negative reactions, allowing users to provide specific improvement suggestions.</li>
  <li><strong>Design smooth conversation endings</strong> that balance user experience with data collection needs.</li>
  <li><strong>Understand analytics insights</strong> to identify which knowledge sources drive highest satisfaction and recognize abandonment patterns.</li>
</ul>

<hr />

<h2 id="-use-cases-covered">🧩 Use Cases Covered</h2>

<table>
  <thead>
    <tr>
      <th>Step</th>
      <th>Use Case</th>
      <th>Value added</th>
      <th>Effort</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td><a href="#-use-case-1-the-end-of-conversation-topic">The end of conversation topic</a></td>
      <td>Effectively manage user interactions by understanding when and how to seamlessly redirect users to the end-of-conversation topic, enabling accurate tracking of conversation outcomes.</td>
      <td>15 min</td>
    </tr>
    <tr>
      <td>2</td>
      <td><a href="#-use-case-2-create-a-smooth-and-intuitive-end-of-conversation-experience">Create a smooth and intuitive end-of-conversation experience</a></td>
      <td>Customize the default end-of-conversation topic to create a more seamless, conversational-friendly experience for users.</td>
      <td>15 min</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="️-instructions-by-use-case">🛠️ Instructions by Use Case</h2>

<hr />

<h2 id="-use-case-1-the-end-of-conversation-topic">🧱 Use Case #1: The end of conversation topic</h2>

<p>Every conversation should have a conclusion – Design for clear outcomes.</p>

<table>
  <thead>
    <tr>
      <th>Use case</th>
      <th>Value added</th>
      <th>Estimated effort</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>The end of conversation topic</td>
      <td>Effectively manage user interactions by understanding when and how to seamlessly redirect users to the end-of-conversation topic, enabling accurate tracking of conversation outcomes.</td>
      <td>15 minutes</td>
    </tr>
  </tbody>
</table>

<p><strong>Summary of tasks</strong></p>

<p>In this section, you’ll learn how the end of conversation topic works in Copilot Studio and how to use it effectively in your conversation design. By structuring conversations for clear outcomes, you’ll enable meaningful analytics and actionable insights to improve your agent’s performance.</p>

<p><strong>Scenario:</strong> You’ve built an agent with the knowledge to answer user questions—but is it actually delivering? If your analytics dashboard isn’t showing meaningful data, it’s time to track successful conversation outcomes and CSAT scores. You can’t improve what you can’t measure.</p>

<h3 id="objective">Objective</h3>

<p>Configure your agent to properly track conversation outcomes by implementing the End of Conversation topic and understanding how it integrates with both classic and generative orchestration modes.</p>

<hr />

<h3 id="step-by-step-instructions">Step-by-step instructions</h3>

<h4 id="understanding-the-end-of-conversation-topic">Understanding the End of Conversation Topic</h4>

<ol>
  <li>
    <p>Navigate to the Copilot Studio agent you have created for this lab (e.g., LAB-10, or a new one).
https://aka.ms/MCSStart</p>
  </li>
  <li>
    <p>Go to the <strong>Topics</strong> tab, display <strong>All</strong>, and select <strong>End of conversation</strong>.</p>
  </li>
  <li>
    <p>Explore what the topic is doing.</p>
  </li>
</ol>

<blockquote>
  <p>[!TIP]
The end of conversation topic is meant to be triggered when the agent has presumably fulfilled the user’s request. This can happen either after providing a direct answer, such as retrieving information from knowledge sources, or after completing a more complex multi-turn interaction where the user and agent exchange multiple messages to complete a task.</p>
</blockquote>

<p>By default, when the conversation reaches this stage, the agent asks, “Did this answer your question?” At this point, the resolution is considered implicit, meaning that if the user leaves without responding, it is assumed that their query was resolved. However, if the user confirms that their question was answered, the resolution becomes explicit, and they are then prompted to provide a Customer Satisfaction Score (CSAT) to rate their experience.</p>

<h4 id="configuring-classic-orchestration-mode">Configuring Classic Orchestration Mode</h4>

<ol>
  <li>
    <p>Explore other topics in your agent. By default, are they redirecting to the End of conversation topic?</p>

    <p>For newly created agents, that shouldn’t be the case. The End of conversation topic must be redirected to explicitly from the places where you feel the user request has been fulfilled.</p>
  </li>
  <li>
    <p>Assuming the Generative mode for orchestration is <strong>Disabled</strong> on your agent (you can see that option either in the Overview tab, or in Settings, under Generative AI).</p>

    <p><img src="images/generative-ai-disabled.png" alt="Generative AI disabled" /></p>
  </li>
  <li>
    <p>Let’s add a redirect to the End of conversation topic from the <strong>Conversational boosting</strong> topic.</p>

    <p>Go to the <strong>Topics</strong> tab, display <strong>All</strong>, and select <strong>Conversational boosting</strong>.</p>

    <p><img src="images/conversational-boosting.png" alt="Conversational boosting topic" /></p>

    <p>Delete the <strong>End current topic</strong> node, and instead, add a new node: <strong>Topic management</strong> &gt; <strong>Go to another topic</strong> &gt; and select <strong>End of Conversation</strong>.</p>

    <p>Save the topic.</p>
  </li>
</ol>

<h4 id="testing-classic-mode-behavior">Testing Classic Mode Behavior</h4>

<ol>
  <li>
    <p>Now, test your agent in the test pane by asking a question that will trigger the Conversational boosting topic.</p>

    <p>For example, you may ask:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What are the key metrics offered by the analytics dashboard?
</code></pre></div>    </div>

    <p>Answer the different questions until you can ask a new question. In the happy path, notice you must answer 3 questions before you can ask a new question.</p>

    <p><img src="images/classic-mode-flow.png" alt="Classic mode conversation flow" /></p>
  </li>
</ol>

<h4 id="configuring-generative-mode">Configuring Generative Mode</h4>

<ol>
  <li>
    <p>Let’s now try what the experience is by toggling the <strong>Generative mode</strong> on. You can enable it either in the Overview tab, or in Settings, under Generative AI.</p>

    <p><img src="images/generative-ai-enabled.png" alt="Generative AI enabled" /></p>
  </li>
  <li>
    <p>Refresh the test pane, and ask the same question.</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What are the key metrics offered by the analytics dashboard?
</code></pre></div>    </div>

    <p>Notice how the experience is different. The Activity map is displayed and shows you the agent’s reasoning based on the user query.</p>
  </li>
</ol>

<blockquote>
  <p>[!IMPORTANT]
In the test pane, notice that you are no longer prompted with “Did this answer your question?”. Why isn’t End of conversation triggered? That is because the Conversational boosting topic wasn’t traversed with generative mode.</p>
</blockquote>

<h4 id="creating-the-plan-complete-topic">Creating the Plan Complete Topic</h4>

<ol>
  <li>
    <p>Go to the <strong>Topics</strong> tab, select <strong>+ Add a topic</strong>, and choose <strong>From blank</strong>.</p>

    <p>Don’t leave it with the default “Untitled” label. Select <strong>Untitled</strong> and change the text to <strong>Plan complete</strong>.</p>

    <p>Then, change the trigger by hovering over the “Triggered by agent” box until the icon to swap the trigger for another type appears. Then scroll down and choose <strong>Plan complete</strong>.</p>

    <p><img src="images/plan-complete-trigger.png" alt="Plan complete trigger" /></p>

    <p>Add a new node and select <strong>Topic management</strong> &gt; <strong>Go to another topic</strong> &gt; <strong>End of Conversation</strong>.</p>

    <p>Save the topic.</p>
  </li>
  <li>
    <p>Now, refresh the test pane, and test your agent again.</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What are the key metrics offered by the analytics dashboard?
</code></pre></div>    </div>

    <p>Notice that after the answer is provided by the agent, the user is prompted for confirmation.</p>

    <p><img src="images/generative-mode-plan-complete.png" alt="Generative mode with plan complete" /></p>
  </li>
</ol>

<h4 id="understanding-topic-behavior">Understanding Topic Behavior</h4>

<ol>
  <li>Refresh the test pane and send a simple, everyday message, like “hello”.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Hi!
</code></pre></div>    </div>

    <p>Notice how the End of conversation topic isn’t triggered. Why is that?</p>

    <p>Open the <strong>Greeting</strong> topic by clicking the edit (✏️) icon.</p>

    <p>Notice how the <strong>End all topics</strong> node prevents this behavior. Any subsequent user messages remain within the same conversation session, as the agent assumes the user’s request hasn’t yet been resolved.</p>

    <p><img src="images/greeting-end-all-topics.png" alt="Greeting topic with end all topics" /></p>
  </li>
  <li>
    <p>For the rest of the lab, you may disable the generative mode.</p>

    <p><img src="images/disable-generative-mode.png" alt="Disable generative mode" /></p>
  </li>
</ol>

<hr />

<h3 id="-congratulations-youve-completed-use-case-1">🏅 Congratulations! You’ve completed Use Case #1!</h3>

<hr />

<h3 id="test-your-understanding">Test your understanding</h3>

<p><strong>Key takeaways:</strong></p>

<ul>
  <li><strong>Conversation resolution tracking</strong> – Redirecting to the end of conversation topic allows you to track successful, abandoned, and escalated interactions.</li>
  <li><strong>Session-based analytics</strong> – Conversations can contain multiple sessions, each with a distinct outcome (resolved, escalated, abandoned).</li>
  <li><strong>Customizing conversation endings</strong> – The end of conversation topic can be tailored to enhance user experience, ensuring smooth and meaningful conversation conclusions.</li>
</ul>

<p><strong>Lessons learned &amp; troubleshooting tips:</strong></p>

<ul>
  <li>If your analytics dashboard is showing too many abandoned sessions, check if conversations properly redirect to the end of conversation topic.</li>
  <li>Analytics dashboards don’t show sessions from your own tests in the test pane. Only the interactions that happened over your deployed channels will show.</li>
  <li>When using conversational boosting, ensure the topic transitions correctly to the end of conversation topic to capture user feedback.</li>
</ul>

<p><strong>Challenge: Apply this to your own use case</strong></p>

<ul>
  <li>How can you integrate clear conversation endings into your existing agent topics?</li>
  <li>Where should you collect user feedback to improve response quality?</li>
  <li>What patterns can you identify in abandoned vs. resolved conversations?</li>
</ul>

<hr />

<hr />

<h2 id="-use-case-2-create-a-smooth-and-intuitive-end-of-conversation-experience">🔄 Use Case #2: Create a smooth and intuitive end-of-conversation experience</h2>

<p>End conversations without friction – create a smooth, unobtrusive way to gather feedback without disrupting the flow.</p>

<table>
  <thead>
    <tr>
      <th>Use case</th>
      <th>Value added</th>
      <th>Estimated effort</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Create a smooth and intuitive end-of-conversation experience</td>
      <td>Customize the default end-of-conversation topic to create a more seamless, conversational-friendly experience for users.</td>
      <td>15 minutes</td>
    </tr>
  </tbody>
</table>

<p><strong>Summary of tasks</strong></p>

<p>In this section, you’ll learn how the default End of conversation topic can unintentionally interrupt the user’s conversational flow, forcing unnecessary feedback prompts or confirmations. You’ll see how to modify this default behavior to create a smoother and more intuitive experience.</p>

<p><strong>Scenario:</strong> Visitors on your website frequently have multiple related questions about products and solutions. The default End of conversation prompt can disrupt their experience by forcing them into providing feedback or acknowledgments prematurely. By customizing the End of conversation topic, you’ll enable a more fluid interaction, allowing users to naturally continue conversations without friction or interruption.</p>

<h3 id="step-by-step-instructions-1">Step-by-step instructions</h3>

<ol>
  <li>Let’s start with a test. Refresh the test pane and ask two questions consecutively.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What is Copilot Studio?
</code></pre></div>    </div>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What knowledge sources does it support?
</code></pre></div>    </div>

    <p>Notice how the default End of conversation topic interrupts the interaction, preventing the second question from being answered until the user responds to the prompt, “Did that answer your question?”</p>

    <p><img src="images/interrupted-flow.png" alt="Interrupted conversation flow" /></p>
  </li>
  <li>
    <p>Go to the <strong>Topics</strong> tab, display <strong>All</strong>, and select <strong>End of conversation</strong>.</p>

    <p>On the question “Did that answer your question?”, select the ellipsis (…) and open properties.</p>

    <p>Go to <strong>Question behavior</strong>.
Set <strong>How many reprompts</strong> to <strong>Don’t repeat</strong>.</p>

    <p>Return to the Question Properties, then select <strong>Entity recognition</strong>:
For <strong>Action if no entity found</strong>, choose <strong>Set variable to empty (no value)</strong>.</p>
  </li>
  <li>
    <p>Below the “Did that answer your question?” question, notice the condition only tests if SurveyResponse is true. Let’s add another condition path, by clicking on the (➕) action above the various conditions.</p>

    <p>In <strong>Select a variable</strong>, choose <strong>SurveyResponse</strong>. For the test, leave it to <strong>is equal to</strong>, and set <strong>false</strong> for the value.</p>

    <p>Move everything that is under <strong>All other conditions</strong> under the new false branch by cutting and pasting the content.</p>
  </li>
  <li>
    <p>Add a redirect to the <strong>Conversational boosting</strong> topic under the <strong>All other conditions</strong> path.</p>

    <p><img src="images/end-conversation-structure.png" alt="End of conversation flow structure" /></p>
  </li>
  <li>
    <p>Save your topic.</p>
  </li>
  <li>Let’s do a new test. Refresh the test pane and ask again two questions consecutively.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What is Copilot Studio?
</code></pre></div>    </div>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What knowledge sources does it support?
</code></pre></div>    </div>

    <p>Notice how follow-up questions are no longer blocking the conversation flow.</p>
  </li>
</ol>

<blockquote>
  <p>[!TIP]
If you want the user’s follow-up questions to trigger existing topics rather than always defaulting to Conversational boosting, verify the Interruptions setting in the “Did that answer your question?” question properties. By default, they allow interruptions, meaning the agent can seamlessly switch to a recognized topic based on the user’s next input.</p>
</blockquote>

<ol>
  <li>
    <p>You can further simplify what happens after the user answers <strong>Yes</strong>.</p>

    <p>After the CSAT question, add a message node asking:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Thank you for your feedback!

Feel free to ask me something else.
</code></pre></div>    </div>

    <p>Then add a node <strong>Topic management</strong> &gt; <strong>End conversation</strong>.</p>

    <p>Delete everything further below that path.</p>
  </li>
  <li>
    <p>You can further simplify what happens after the user answers <strong>No</strong>.</p>

    <p>Under the <strong>SurveyResponse false</strong> condition path, add a new message node:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sorry I wasn't able to help better.

You may try reaching out to our [Microsoft Copilot Studio community](https://aka.ms/CopilotStudioCommunity) or submitting a [support request](https://learn.microsoft.com/en-us/power-platform/admin/get-help-support).

Would you like to try again? Feel free to ask a new question.
</code></pre></div>    </div>

    <p>Then add a node <strong>Topic management</strong> &gt; <strong>End conversation</strong>.</p>

    <p>Save topic.</p>
  </li>
</ol>

<hr />

<h3 id="-congratulations-youve-completed-use-case-2">🏅 Congratulations! You’ve completed Use Case #2!</h3>

<hr />

<h3 id="test-your-understanding-1">Test your understanding</h3>

<ul>
  <li>How does customizing the End of Conversation topic improve user experience without sacrificing data collection?</li>
  <li>What are the trade-offs between gathering comprehensive feedback and maintaining conversation flow?</li>
  <li>How can you apply these principles to create more natural conversation endings in your own agents?</li>
</ul>

<p><strong>Challenge: Apply this to your own use case</strong></p>

<ul>
  <li>Where in your current agent can users experience unnecessary conversational friction?</li>
  <li>How can you apply these customization strategies to ensure smoother conversation endings?</li>
  <li>In what ways can user feedback be naturally integrated without interrupting conversational flow?</li>
</ul>

<hr />

<h2 id="-summary-of-learnings">🏆 Summary of learnings</h2>

<p>True learning comes from doing, questioning, and reflecting—so let’s put your skills to the test.</p>

<p>To maximize the impact of conversation outcome tracking:</p>

<ul>
  <li><strong>Structure conversations for clear outcomes</strong> – Every interaction should have a definitive end point that can be measured and analyzed for continuous improvement.</li>
  <li><strong>Balance user experience with data collection</strong> – Gather meaningful feedback without creating friction that drives users away from your agent.</li>
  <li><strong>Implement session-based tracking</strong> – Understand that individual queries within conversations have separate outcomes that provide granular insights.</li>
  <li><strong>Leverage both implicit and explicit resolution</strong> – Track when users are satisfied enough to leave (implicit) versus when they explicitly confirm satisfaction.</li>
  <li><strong>Design for different orchestration modes</strong> – Ensure your tracking works effectively whether using classic topics or generative AI responses.</li>
</ul>

<hr />

<h3 id="conclusions-and-recommendations">Conclusions and recommendations</h3>

<p><strong>Conversation analytics golden rules:</strong></p>

<ul>
  <li>Always redirect to the End of Conversation topic when user requests are fulfilled to ensure accurate outcome tracking.</li>
  <li>Customize feedback prompts to match your users’ communication style and reduce abandonment rates.</li>
  <li>Test conversation flows regularly to identify where users experience friction or confusion.</li>
  <li>Use analytics insights to identify knowledge gaps and refine content strategy continuously.</li>
  <li>Balance comprehensive data collection with user experience to maintain high engagement levels.</li>
  <li>Implement both lightweight feedback (thumbs up/down) and detailed feedback collection for comprehensive insights.</li>
</ul>

<p>By following these principles, you’ll transform your AI agent from a simple response tool into a data-driven system that continuously learns and improves, delivering measurably better user experiences and business outcomes.</p>

<hr />


    </div>

    <footer class="lab-footer">
        <div class="lab-navigation">
            
            
            
            
            
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                    
                    
            
            
                <a href="/mcs-labs/labs/copilot-studio-kit/" class="nav-button prev">
                    ← Copilot Studio Kit
                </a>
            
            
            
                <a href="/mcs-labs/labs/mcp-qualify-lead/" class="nav-button next">
                    Model Context Protocol Integration →
                </a>
            
        </div>
    </footer>
</article>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p>&copy; 2025 Microsoft Copilot Studio Labs</p>
        </div>
    </footer>
</body>
</html>