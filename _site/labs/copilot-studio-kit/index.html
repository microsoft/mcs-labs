<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Copilot Studio Kit | Microsoft Copilot Studio Labs</title>
    <link rel="stylesheet" href="/mcs-labs/assets/css/style.css">
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Copilot Studio Kit | Microsoft Copilot Studio Labs</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Copilot Studio Kit" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Deliver high-quality, scalable agents with Copilot Studio Kit Automate quality assurance for your Copilot Studio agents with the Copilot Studio Kit. In this lab, you’ll configure and run automated test scenarios, validate agent behavior using multiple test types, and gain actionable insights through analytics. You’ll also explore advanced tools like conversation KPIs, UI customization, and governance dashboards—equipping you to ship high-quality, secure, and scalable AI agents with confidence. 🧭 Lab Details Level Persona Duration Purpose 200 Developer/Maker 30 minutes After completing this lab, participants will be able to configure automated testing for Copilot Studio agents, create comprehensive test suites, and analyze agent performance through systematic validation. Gain expertise in quality assurance practices for AI-powered conversational agents. 📚 Table of Contents Why This Matters Introduction Core Concepts Overview Documentation and Additional Training Links Prerequisites Summary of Targets Use Cases Covered Instructions by Use Case Use Case #1: Configure Agent Testis Use Case #2: Execute and Analyze Test Results Use Case #3: Explore the other Copilot Studio Kit features 🤔 Why This Matters Makers and Developers - Worried about deploying AI agents that might give inconsistent or incorrect responses to users? Think of testing a chatbot like quality control in manufacturing: Without systematic testing: Agents deployed with unknown behavior, potential for embarrassing mistakes, and no confidence in responses With automated testing: Validated agent performance, consistent quality assurance, and data-driven optimization insights Common challenges solved by this lab: “How do I know if my agent is giving good answers?” “I need to test multiple scenarios without manually chatting hundreds of times” “My agent works sometimes but fails on edge cases I didn’t think of” “I want to track performance improvements over time” Spend 30 minutes now to save hours of manual testing and avoid costly production issues later. 🌐 Introduction Quality assurance for AI agents requires a systematic approach that goes beyond manual testing. The Power CAT Copilot Studio Kit provides enterprise-grade testing capabilities that enable makers to validate agent behavior at scale. Real-world example: A customer service organization deploys a Copilot Studio agent to handle common inquiries about product returns. Without proper testing, the agent might incorrectly handle edge cases like partial returns or international shipping, leading to frustrated customers and escalated support tickets. With automated testing, they can validate hundreds of scenarios before deployment and catch issues early. This lab teaches you to implement professional testing practices that ensure your agents deliver consistent, reliable experiences to end users. 🎓 Core Concepts Overview Concept Why it matters Agent Configuration Defines the connection between your Copilot Studio agent and the testing framework, enabling automated interactions and response validation Test Sets and Test Cases Structured collections of scenarios that systematically validate agent behavior across different conversation patterns and edge cases Test Types Different validation approaches (response &amp; topic match, generative AI analysis, adaptive cards) that ensure comprehensive coverage of agent capabilities Test Runs and Analytics Execution framework that provides detailed performance metrics, response analysis, and trending data for continuous improvement 📄 Documentation and Additional Training Links Power CAT Copilot Studio Kit Documentation ✅ Prerequisites Access to a Microsoft Copilot Studio environment with agent creation permissions. Access to the Power CAT Copilot Studio Kit (installed from AppSource or GitHub). 🎯 Summary of Targets In this lab, you’ll establish a comprehensive testing framework for Copilot Studio agents. By the end of the lab, you will: Set up agent configuration in the Copilot Studio Kit. Create structured test sets with multiple validation scenarios including exact matches and AI-powered response analysis. Execute automated test runs and interpret detailed insights. Explore other features of the kit. 🧩 Use Cases Covered Step Use Case Value added Effort 1 Configure Agent Tests Establishes automated testing pipeline for systematic agent validation 10 min 2 Execute and Analyze Test Results Delivers actionable insights for agent performance optimization 10 min 3 Explore the other Copilot Studio Kit features Beyond testing, the Power CAT Copilot Studio Kit includes powerful tools for real-world deployment and governance: conversation KPIs for tracking agent performance, SharePoint synchronization for auto-updating knowledge bases, the Webchat Playground for UI customization, and the Adaptive Cards Gallery for rich interactions. It also offers admin tools like Agent Inventory for environment-wide visibility and the Agent Review Tool to detect issues and anti-patterns—helping makers ship secure, scalable, and effective agents. 10 min 🛠️ Instructions by Use Case 🧱 Use Case #1: Configure Agent Tests Set up the foundational components needed to automate testing of your Copilot Studio agents. Use case Value added Estimated effort Configure Agent Testing Infrastructure Establishes automated testing pipeline for systematic agent validation 20 minutes Summary of tasks In this section, you’ll learn how to configure an external-facing agent for testing, connect it to the Power CAT Copilot Studio Kit, and create comprehensive test scenarios. Scenario: You’re a maker responsible for a customer support agent that handles product inquiries. Before deploying to production, you need to ensure the agent responds appropriately to various customer questions and edge cases. Objective Configure a Copilot Studio agent for external access and integrate it with the Power CAT testing framework to enable automated validation. Step-by-step instructions Access the Copilot Studio Kit application Start by accessing the Copilot Studio Kit application, either installed in your organization or the one that was setup for the training. [!IMPORTANT] To access this app, use the provided URL in the Lab Resources (specific per training). The first time you open the app, you will be prompted to allow Copilot Studio Kit to access your data. Select Allow to grant permissions. Create an agent configuration Go to Agents in the left navigation menu. Select New Name it Nova AI and select Test Automation as Configuration Type. To obtain the Token Endpoint, you need to navigate to your Nova AI agent, go to Channels, select Direct Line Speech, and copy the Token Endpoint URL. Paste this URL into the Token Endpoint field in the Copilot Studio Kit. [!TIP] The tokent endpoint allows to obtain a token that can be used to initiate a conversation with the agent programmatically. This is essential for automated testing frameworks to interact with your agent. Once the token is obtained, it can be used to send messages to the agent and receive responses from the Direct Line API. [!IMPORTANT] Using “No authentication” is suitable for agents that do not access sensitive internal information. It allows anyone to interact with your agent, so use it cautiously. Select Save and close to create the agent configuration. Create a Test Set Navigate to Test Sets and click New to create a new test set for your agent. Name the test set Nova AI Test Set and Save. You can now add + New Agent Test in the subgrid. Test Set Name Type Test Utterance Expected Response TST-001 Response Match Hi! Hello, how can I help you today? TST-002 Generative Answer What caused the fall of the Roman Empire? The fall of the Roman Empire was a gradual process driven by internal issues like political instability and economic decline, and external pressures such as barbarian invasions and climate change. [!TIP] You can either add each test individually, or toggle the subgrid view to Export/Import View and select selecting ⋮ Export Agent Tests &gt; Export Agent Tests in Excel Online. This makes it very convenient to create or update multiple tests at once, especially for larger test sets. 🏅 Congratulations! You’ve completed Agent Testing Infrastructure Setup! Test your understanding Key takeaways: Authentication Configuration – Setting “No authentication” enables automated testing but should only be used for external-facing agents without sensitive data access. It is possible to confiure authentication for internal agents, but this requires additional setup. Token Endpoints – These URLs provide programmatic access to your agent and are essential for automated testing frameworks Test Set Design – Comprehensive test suites include exact matches, AI-powered analysis, and topic validation to ensure thorough coverage Lessons learned &amp; troubleshooting tips: If the token endpoint doesn’t work, ensure your agent is fully published and the authentication settings are saved Test with simple questions first before creating complex multi-turn scenarios Remember that “No authentication” agents are publicly accessible, so avoid including sensitive test data Challenge: Apply this to your own use case What types of questions do your users most commonly ask that you should include in test cases? How would you structure test scenarios to cover both happy path and error handling situations? What metrics would be most valuable for measuring your agent’s success in your specific business context? 🔄 Use Case #2: Execute and Analyze Test Results Run your configured tests and interpret the results to optimize agent performance and ensure quality. Use case Value added Estimated effort Execute and Analyze Test Results Delivers actionable insights for agent performance optimization 10 minutes Summary of tasks In this section, you’ll learn how to execute test runs, analyze detailed results including response quality and performance metrics, and use insights for continuous improvement. Scenario: Your test infrastructure is configured and you need to validate your agent’s performance across multiple scenarios. You want to establish baseline metrics and identify areas for improvement before full deployment. Step-by-step instructions Execute Test Runs Navigate to Test Runs in the Copilot Studio Kit and click New to create a new test execution. Name it Nova AI Test Run 01 Select the Agent Configuration** and Agent Test Set configured in the previous use case. Save the test run to initialize it. [!TIP] Test runs can take several minutes depending on the number of test cases and agent response times. You can monitor progress in real-time. Analyze Test Results and Performance Metrics Review the Test Run Results page which displays comprehensive analytics including: Overall pass/fail rates for your test set Individual test case results with actual vs. expected responses Response latency measurements for performance optimization Detailed AI analysis scores for generative response tests Examine failed test cases by clicking on individual results to understand why responses didn’t meet expectations. [!IMPORTANT] Pay special attention to tests that fail due to response quality rather than exact matching. These often indicate opportunities for prompt optimization or knowledge base improvements. Optimize Based on Insights Identify patterns in test failures such as: Specific topic areas where the agent struggles Response time issues that might impact user experience Inconsistent handling of similar questions Document recommended improvements based on test results, such as: Knowledge base updates for failed content questions Prompt engineering adjustments for better response quality Additional training data needs for improved accuracy [!TIP] Create a feedback loop by implementing improvements in your agent, then re-running the same test set to measure progress over time. 🏅 Congratulations! You’ve completed Test Execution and Analysis! Test your understanding How do the different test types (exact match vs. generative AI analysis) provide complementary insights into agent performance? What patterns in test results would indicate that your agent needs additional training data versus prompt optimization? How could you use test results to demonstrate ROI and continuous improvement to business stakeholders? Challenge: Apply this to your own use case Set up a regular testing schedule that aligns with your agent development and deployment cycles Create test scenarios that reflect your actual user conversations and business requirements Establish key performance indicators (KPIs) based on test results that align with your business goals 🎨 Use Case #3: Explore Advanced Kit Features Discover the comprehensive ecosystem of tools available in the Power CAT Copilot Studio Kit for enhanced agent development and management. Use case Value added Estimated effort Explore Advanced Kit Features Discovers comprehensive toolkit capabilities for enhanced agent development 10 minutes Summary of tasks In this section, you’ll explore the full range of capabilities beyond testing, including conversation analytics, SharePoint integration, UI customization, and administrative tools. Scenario: You’ve successfully implemented testing for your agents and want to understand what other capabilities are available to enhance your agent development lifecycle, improve user experience, and streamline administrative tasks. Step-by-step instructions Explore Conversation KPIs and Analytics Navigate to Conversation KPIs in the Power CAT Kit to explore advanced analytics capabilities that complement Copilot Studio’s built-in analytics. Review the dashboard features that provide aggregated conversation data in Dataverse, making it easier to understand conversation outcomes without analyzing complex transcripts. [!TIP] Conversation KPIs simplify performance tracking by providing structured data that can be easily exported and analyzed in Power BI or other reporting tools. Review Governance Tools Explore Agent Inventory which provides administrators with a tenant-wide view of all Copilot Studio agents, including usage patterns and configuration details. Investigate User Experience Tools Visit the Webchat Playground to explore visual customization options for your agent’s chat interface, including colors, fonts, and branding elements. Examine the Adaptive Cards Gallery which provides pre-built templates for rich interactive responses that enhance user engagement. [!TIP] Use the Webchat Playground to generate HTML with custom styles that match your organization’s branding guidelines. 🏅 Congratulations! You’ve explored the full Power CAT Copilot Studio Kit ecosystem! Test your understanding How do the various tools in the kit complement each other to provide a complete agent development lifecycle? Which administrative tools would be most valuable for maintaining governance and security in a large organization? How could you integrate these capabilities into your existing development and deployment processes? Challenge: Apply this to your own use case Identify which additional kit features would provide the most value for your specific agent development scenarios Plan how you could incorporate conversation KPIs and analytics into your ongoing agent optimization process Consider how administrative tools could help you scale agent development across your organization while maintaining quality and security standards 🏆 Summary of learnings True learning comes from doing, questioning, and reflecting—so let’s put your skills to the test. To maximize the impact of automated agent testing and the Power CAT ecosystem: Comprehensive Test Coverage – Include exact matches, AI-powered analysis, and edge cases to ensure thorough validation of agent behavior across all scenarios Regular Testing Cycles – Establish automated testing as part of your development workflow to catch issues early and track improvements over time Data-Driven Optimization – Use test results and conversation KPIs to make informed decisions about prompt engineering, knowledge base updates, and conversation flow improvements Performance Monitoring – Track response times and quality metrics to ensure your agent meets both functional and performance requirements Ecosystem Integration – Leverage the full range of kit capabilities including SharePoint synchronization, UI customization, and administrative tools for comprehensive agent lifecycle management Governance and Security – Use administrative tools like Agent Inventory and Agent Review Tool to maintain standards and security across your organization’s agent portfolio Conclusions and recommendations Copilot Studio Kit golden rules: Test early and test often - integrate automated testing into your development workflow from the beginning Cover both happy paths and edge cases - real users will ask unexpected questions that reveal gaps in agent training Use multiple test types for comprehensive validation - combine exact matching with AI-powered analysis for complete coverage Monitor performance trends over time - establish baselines and track improvements as you optimize your agent Implement proper governance - use the inventory tools to maintain security, compliance, and quality standards across all agents Document and share results - use test data and analytics to build confidence with stakeholders and guide business decisions Maintain test sets as living documents - update scenarios based on real user interactions and changing business needs By following these principles, you’ll deliver reliable, high-quality conversational AI experiences that consistently meet user expectations and business objectives while maintaining proper governance and security standards across your organization’s agent portfolio." />
<meta property="og:description" content="Deliver high-quality, scalable agents with Copilot Studio Kit Automate quality assurance for your Copilot Studio agents with the Copilot Studio Kit. In this lab, you’ll configure and run automated test scenarios, validate agent behavior using multiple test types, and gain actionable insights through analytics. You’ll also explore advanced tools like conversation KPIs, UI customization, and governance dashboards—equipping you to ship high-quality, secure, and scalable AI agents with confidence. 🧭 Lab Details Level Persona Duration Purpose 200 Developer/Maker 30 minutes After completing this lab, participants will be able to configure automated testing for Copilot Studio agents, create comprehensive test suites, and analyze agent performance through systematic validation. Gain expertise in quality assurance practices for AI-powered conversational agents. 📚 Table of Contents Why This Matters Introduction Core Concepts Overview Documentation and Additional Training Links Prerequisites Summary of Targets Use Cases Covered Instructions by Use Case Use Case #1: Configure Agent Testis Use Case #2: Execute and Analyze Test Results Use Case #3: Explore the other Copilot Studio Kit features 🤔 Why This Matters Makers and Developers - Worried about deploying AI agents that might give inconsistent or incorrect responses to users? Think of testing a chatbot like quality control in manufacturing: Without systematic testing: Agents deployed with unknown behavior, potential for embarrassing mistakes, and no confidence in responses With automated testing: Validated agent performance, consistent quality assurance, and data-driven optimization insights Common challenges solved by this lab: “How do I know if my agent is giving good answers?” “I need to test multiple scenarios without manually chatting hundreds of times” “My agent works sometimes but fails on edge cases I didn’t think of” “I want to track performance improvements over time” Spend 30 minutes now to save hours of manual testing and avoid costly production issues later. 🌐 Introduction Quality assurance for AI agents requires a systematic approach that goes beyond manual testing. The Power CAT Copilot Studio Kit provides enterprise-grade testing capabilities that enable makers to validate agent behavior at scale. Real-world example: A customer service organization deploys a Copilot Studio agent to handle common inquiries about product returns. Without proper testing, the agent might incorrectly handle edge cases like partial returns or international shipping, leading to frustrated customers and escalated support tickets. With automated testing, they can validate hundreds of scenarios before deployment and catch issues early. This lab teaches you to implement professional testing practices that ensure your agents deliver consistent, reliable experiences to end users. 🎓 Core Concepts Overview Concept Why it matters Agent Configuration Defines the connection between your Copilot Studio agent and the testing framework, enabling automated interactions and response validation Test Sets and Test Cases Structured collections of scenarios that systematically validate agent behavior across different conversation patterns and edge cases Test Types Different validation approaches (response &amp; topic match, generative AI analysis, adaptive cards) that ensure comprehensive coverage of agent capabilities Test Runs and Analytics Execution framework that provides detailed performance metrics, response analysis, and trending data for continuous improvement 📄 Documentation and Additional Training Links Power CAT Copilot Studio Kit Documentation ✅ Prerequisites Access to a Microsoft Copilot Studio environment with agent creation permissions. Access to the Power CAT Copilot Studio Kit (installed from AppSource or GitHub). 🎯 Summary of Targets In this lab, you’ll establish a comprehensive testing framework for Copilot Studio agents. By the end of the lab, you will: Set up agent configuration in the Copilot Studio Kit. Create structured test sets with multiple validation scenarios including exact matches and AI-powered response analysis. Execute automated test runs and interpret detailed insights. Explore other features of the kit. 🧩 Use Cases Covered Step Use Case Value added Effort 1 Configure Agent Tests Establishes automated testing pipeline for systematic agent validation 10 min 2 Execute and Analyze Test Results Delivers actionable insights for agent performance optimization 10 min 3 Explore the other Copilot Studio Kit features Beyond testing, the Power CAT Copilot Studio Kit includes powerful tools for real-world deployment and governance: conversation KPIs for tracking agent performance, SharePoint synchronization for auto-updating knowledge bases, the Webchat Playground for UI customization, and the Adaptive Cards Gallery for rich interactions. It also offers admin tools like Agent Inventory for environment-wide visibility and the Agent Review Tool to detect issues and anti-patterns—helping makers ship secure, scalable, and effective agents. 10 min 🛠️ Instructions by Use Case 🧱 Use Case #1: Configure Agent Tests Set up the foundational components needed to automate testing of your Copilot Studio agents. Use case Value added Estimated effort Configure Agent Testing Infrastructure Establishes automated testing pipeline for systematic agent validation 20 minutes Summary of tasks In this section, you’ll learn how to configure an external-facing agent for testing, connect it to the Power CAT Copilot Studio Kit, and create comprehensive test scenarios. Scenario: You’re a maker responsible for a customer support agent that handles product inquiries. Before deploying to production, you need to ensure the agent responds appropriately to various customer questions and edge cases. Objective Configure a Copilot Studio agent for external access and integrate it with the Power CAT testing framework to enable automated validation. Step-by-step instructions Access the Copilot Studio Kit application Start by accessing the Copilot Studio Kit application, either installed in your organization or the one that was setup for the training. [!IMPORTANT] To access this app, use the provided URL in the Lab Resources (specific per training). The first time you open the app, you will be prompted to allow Copilot Studio Kit to access your data. Select Allow to grant permissions. Create an agent configuration Go to Agents in the left navigation menu. Select New Name it Nova AI and select Test Automation as Configuration Type. To obtain the Token Endpoint, you need to navigate to your Nova AI agent, go to Channels, select Direct Line Speech, and copy the Token Endpoint URL. Paste this URL into the Token Endpoint field in the Copilot Studio Kit. [!TIP] The tokent endpoint allows to obtain a token that can be used to initiate a conversation with the agent programmatically. This is essential for automated testing frameworks to interact with your agent. Once the token is obtained, it can be used to send messages to the agent and receive responses from the Direct Line API. [!IMPORTANT] Using “No authentication” is suitable for agents that do not access sensitive internal information. It allows anyone to interact with your agent, so use it cautiously. Select Save and close to create the agent configuration. Create a Test Set Navigate to Test Sets and click New to create a new test set for your agent. Name the test set Nova AI Test Set and Save. You can now add + New Agent Test in the subgrid. Test Set Name Type Test Utterance Expected Response TST-001 Response Match Hi! Hello, how can I help you today? TST-002 Generative Answer What caused the fall of the Roman Empire? The fall of the Roman Empire was a gradual process driven by internal issues like political instability and economic decline, and external pressures such as barbarian invasions and climate change. [!TIP] You can either add each test individually, or toggle the subgrid view to Export/Import View and select selecting ⋮ Export Agent Tests &gt; Export Agent Tests in Excel Online. This makes it very convenient to create or update multiple tests at once, especially for larger test sets. 🏅 Congratulations! You’ve completed Agent Testing Infrastructure Setup! Test your understanding Key takeaways: Authentication Configuration – Setting “No authentication” enables automated testing but should only be used for external-facing agents without sensitive data access. It is possible to confiure authentication for internal agents, but this requires additional setup. Token Endpoints – These URLs provide programmatic access to your agent and are essential for automated testing frameworks Test Set Design – Comprehensive test suites include exact matches, AI-powered analysis, and topic validation to ensure thorough coverage Lessons learned &amp; troubleshooting tips: If the token endpoint doesn’t work, ensure your agent is fully published and the authentication settings are saved Test with simple questions first before creating complex multi-turn scenarios Remember that “No authentication” agents are publicly accessible, so avoid including sensitive test data Challenge: Apply this to your own use case What types of questions do your users most commonly ask that you should include in test cases? How would you structure test scenarios to cover both happy path and error handling situations? What metrics would be most valuable for measuring your agent’s success in your specific business context? 🔄 Use Case #2: Execute and Analyze Test Results Run your configured tests and interpret the results to optimize agent performance and ensure quality. Use case Value added Estimated effort Execute and Analyze Test Results Delivers actionable insights for agent performance optimization 10 minutes Summary of tasks In this section, you’ll learn how to execute test runs, analyze detailed results including response quality and performance metrics, and use insights for continuous improvement. Scenario: Your test infrastructure is configured and you need to validate your agent’s performance across multiple scenarios. You want to establish baseline metrics and identify areas for improvement before full deployment. Step-by-step instructions Execute Test Runs Navigate to Test Runs in the Copilot Studio Kit and click New to create a new test execution. Name it Nova AI Test Run 01 Select the Agent Configuration** and Agent Test Set configured in the previous use case. Save the test run to initialize it. [!TIP] Test runs can take several minutes depending on the number of test cases and agent response times. You can monitor progress in real-time. Analyze Test Results and Performance Metrics Review the Test Run Results page which displays comprehensive analytics including: Overall pass/fail rates for your test set Individual test case results with actual vs. expected responses Response latency measurements for performance optimization Detailed AI analysis scores for generative response tests Examine failed test cases by clicking on individual results to understand why responses didn’t meet expectations. [!IMPORTANT] Pay special attention to tests that fail due to response quality rather than exact matching. These often indicate opportunities for prompt optimization or knowledge base improvements. Optimize Based on Insights Identify patterns in test failures such as: Specific topic areas where the agent struggles Response time issues that might impact user experience Inconsistent handling of similar questions Document recommended improvements based on test results, such as: Knowledge base updates for failed content questions Prompt engineering adjustments for better response quality Additional training data needs for improved accuracy [!TIP] Create a feedback loop by implementing improvements in your agent, then re-running the same test set to measure progress over time. 🏅 Congratulations! You’ve completed Test Execution and Analysis! Test your understanding How do the different test types (exact match vs. generative AI analysis) provide complementary insights into agent performance? What patterns in test results would indicate that your agent needs additional training data versus prompt optimization? How could you use test results to demonstrate ROI and continuous improvement to business stakeholders? Challenge: Apply this to your own use case Set up a regular testing schedule that aligns with your agent development and deployment cycles Create test scenarios that reflect your actual user conversations and business requirements Establish key performance indicators (KPIs) based on test results that align with your business goals 🎨 Use Case #3: Explore Advanced Kit Features Discover the comprehensive ecosystem of tools available in the Power CAT Copilot Studio Kit for enhanced agent development and management. Use case Value added Estimated effort Explore Advanced Kit Features Discovers comprehensive toolkit capabilities for enhanced agent development 10 minutes Summary of tasks In this section, you’ll explore the full range of capabilities beyond testing, including conversation analytics, SharePoint integration, UI customization, and administrative tools. Scenario: You’ve successfully implemented testing for your agents and want to understand what other capabilities are available to enhance your agent development lifecycle, improve user experience, and streamline administrative tasks. Step-by-step instructions Explore Conversation KPIs and Analytics Navigate to Conversation KPIs in the Power CAT Kit to explore advanced analytics capabilities that complement Copilot Studio’s built-in analytics. Review the dashboard features that provide aggregated conversation data in Dataverse, making it easier to understand conversation outcomes without analyzing complex transcripts. [!TIP] Conversation KPIs simplify performance tracking by providing structured data that can be easily exported and analyzed in Power BI or other reporting tools. Review Governance Tools Explore Agent Inventory which provides administrators with a tenant-wide view of all Copilot Studio agents, including usage patterns and configuration details. Investigate User Experience Tools Visit the Webchat Playground to explore visual customization options for your agent’s chat interface, including colors, fonts, and branding elements. Examine the Adaptive Cards Gallery which provides pre-built templates for rich interactive responses that enhance user engagement. [!TIP] Use the Webchat Playground to generate HTML with custom styles that match your organization’s branding guidelines. 🏅 Congratulations! You’ve explored the full Power CAT Copilot Studio Kit ecosystem! Test your understanding How do the various tools in the kit complement each other to provide a complete agent development lifecycle? Which administrative tools would be most valuable for maintaining governance and security in a large organization? How could you integrate these capabilities into your existing development and deployment processes? Challenge: Apply this to your own use case Identify which additional kit features would provide the most value for your specific agent development scenarios Plan how you could incorporate conversation KPIs and analytics into your ongoing agent optimization process Consider how administrative tools could help you scale agent development across your organization while maintaining quality and security standards 🏆 Summary of learnings True learning comes from doing, questioning, and reflecting—so let’s put your skills to the test. To maximize the impact of automated agent testing and the Power CAT ecosystem: Comprehensive Test Coverage – Include exact matches, AI-powered analysis, and edge cases to ensure thorough validation of agent behavior across all scenarios Regular Testing Cycles – Establish automated testing as part of your development workflow to catch issues early and track improvements over time Data-Driven Optimization – Use test results and conversation KPIs to make informed decisions about prompt engineering, knowledge base updates, and conversation flow improvements Performance Monitoring – Track response times and quality metrics to ensure your agent meets both functional and performance requirements Ecosystem Integration – Leverage the full range of kit capabilities including SharePoint synchronization, UI customization, and administrative tools for comprehensive agent lifecycle management Governance and Security – Use administrative tools like Agent Inventory and Agent Review Tool to maintain standards and security across your organization’s agent portfolio Conclusions and recommendations Copilot Studio Kit golden rules: Test early and test often - integrate automated testing into your development workflow from the beginning Cover both happy paths and edge cases - real users will ask unexpected questions that reveal gaps in agent training Use multiple test types for comprehensive validation - combine exact matching with AI-powered analysis for complete coverage Monitor performance trends over time - establish baselines and track improvements as you optimize your agent Implement proper governance - use the inventory tools to maintain security, compliance, and quality standards across all agents Document and share results - use test data and analytics to build confidence with stakeholders and guide business decisions Maintain test sets as living documents - update scenarios based on real user interactions and changing business needs By following these principles, you’ll deliver reliable, high-quality conversational AI experiences that consistently meet user expectations and business objectives while maintaining proper governance and security standards across your organization’s agent portfolio." />
<link rel="canonical" href="http://0.0.0.0:4000/mcs-labs/labs/copilot-studio-kit/" />
<meta property="og:url" content="http://0.0.0.0:4000/mcs-labs/labs/copilot-studio-kit/" />
<meta property="og:site_name" content="Microsoft Copilot Studio Labs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-10-08T15:03:33-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Copilot Studio Kit" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-10-08T15:03:33-05:00","datePublished":"2025-10-08T15:03:33-05:00","description":"Deliver high-quality, scalable agents with Copilot Studio Kit Automate quality assurance for your Copilot Studio agents with the Copilot Studio Kit. In this lab, you’ll configure and run automated test scenarios, validate agent behavior using multiple test types, and gain actionable insights through analytics. You’ll also explore advanced tools like conversation KPIs, UI customization, and governance dashboards—equipping you to ship high-quality, secure, and scalable AI agents with confidence. 🧭 Lab Details Level Persona Duration Purpose 200 Developer/Maker 30 minutes After completing this lab, participants will be able to configure automated testing for Copilot Studio agents, create comprehensive test suites, and analyze agent performance through systematic validation. Gain expertise in quality assurance practices for AI-powered conversational agents. 📚 Table of Contents Why This Matters Introduction Core Concepts Overview Documentation and Additional Training Links Prerequisites Summary of Targets Use Cases Covered Instructions by Use Case Use Case #1: Configure Agent Testis Use Case #2: Execute and Analyze Test Results Use Case #3: Explore the other Copilot Studio Kit features 🤔 Why This Matters Makers and Developers - Worried about deploying AI agents that might give inconsistent or incorrect responses to users? Think of testing a chatbot like quality control in manufacturing: Without systematic testing: Agents deployed with unknown behavior, potential for embarrassing mistakes, and no confidence in responses With automated testing: Validated agent performance, consistent quality assurance, and data-driven optimization insights Common challenges solved by this lab: “How do I know if my agent is giving good answers?” “I need to test multiple scenarios without manually chatting hundreds of times” “My agent works sometimes but fails on edge cases I didn’t think of” “I want to track performance improvements over time” Spend 30 minutes now to save hours of manual testing and avoid costly production issues later. 🌐 Introduction Quality assurance for AI agents requires a systematic approach that goes beyond manual testing. The Power CAT Copilot Studio Kit provides enterprise-grade testing capabilities that enable makers to validate agent behavior at scale. Real-world example: A customer service organization deploys a Copilot Studio agent to handle common inquiries about product returns. Without proper testing, the agent might incorrectly handle edge cases like partial returns or international shipping, leading to frustrated customers and escalated support tickets. With automated testing, they can validate hundreds of scenarios before deployment and catch issues early. This lab teaches you to implement professional testing practices that ensure your agents deliver consistent, reliable experiences to end users. 🎓 Core Concepts Overview Concept Why it matters Agent Configuration Defines the connection between your Copilot Studio agent and the testing framework, enabling automated interactions and response validation Test Sets and Test Cases Structured collections of scenarios that systematically validate agent behavior across different conversation patterns and edge cases Test Types Different validation approaches (response &amp; topic match, generative AI analysis, adaptive cards) that ensure comprehensive coverage of agent capabilities Test Runs and Analytics Execution framework that provides detailed performance metrics, response analysis, and trending data for continuous improvement 📄 Documentation and Additional Training Links Power CAT Copilot Studio Kit Documentation ✅ Prerequisites Access to a Microsoft Copilot Studio environment with agent creation permissions. Access to the Power CAT Copilot Studio Kit (installed from AppSource or GitHub). 🎯 Summary of Targets In this lab, you’ll establish a comprehensive testing framework for Copilot Studio agents. By the end of the lab, you will: Set up agent configuration in the Copilot Studio Kit. Create structured test sets with multiple validation scenarios including exact matches and AI-powered response analysis. Execute automated test runs and interpret detailed insights. Explore other features of the kit. 🧩 Use Cases Covered Step Use Case Value added Effort 1 Configure Agent Tests Establishes automated testing pipeline for systematic agent validation 10 min 2 Execute and Analyze Test Results Delivers actionable insights for agent performance optimization 10 min 3 Explore the other Copilot Studio Kit features Beyond testing, the Power CAT Copilot Studio Kit includes powerful tools for real-world deployment and governance: conversation KPIs for tracking agent performance, SharePoint synchronization for auto-updating knowledge bases, the Webchat Playground for UI customization, and the Adaptive Cards Gallery for rich interactions. It also offers admin tools like Agent Inventory for environment-wide visibility and the Agent Review Tool to detect issues and anti-patterns—helping makers ship secure, scalable, and effective agents. 10 min 🛠️ Instructions by Use Case 🧱 Use Case #1: Configure Agent Tests Set up the foundational components needed to automate testing of your Copilot Studio agents. Use case Value added Estimated effort Configure Agent Testing Infrastructure Establishes automated testing pipeline for systematic agent validation 20 minutes Summary of tasks In this section, you’ll learn how to configure an external-facing agent for testing, connect it to the Power CAT Copilot Studio Kit, and create comprehensive test scenarios. Scenario: You’re a maker responsible for a customer support agent that handles product inquiries. Before deploying to production, you need to ensure the agent responds appropriately to various customer questions and edge cases. Objective Configure a Copilot Studio agent for external access and integrate it with the Power CAT testing framework to enable automated validation. Step-by-step instructions Access the Copilot Studio Kit application Start by accessing the Copilot Studio Kit application, either installed in your organization or the one that was setup for the training. [!IMPORTANT] To access this app, use the provided URL in the Lab Resources (specific per training). The first time you open the app, you will be prompted to allow Copilot Studio Kit to access your data. Select Allow to grant permissions. Create an agent configuration Go to Agents in the left navigation menu. Select New Name it Nova AI and select Test Automation as Configuration Type. To obtain the Token Endpoint, you need to navigate to your Nova AI agent, go to Channels, select Direct Line Speech, and copy the Token Endpoint URL. Paste this URL into the Token Endpoint field in the Copilot Studio Kit. [!TIP] The tokent endpoint allows to obtain a token that can be used to initiate a conversation with the agent programmatically. This is essential for automated testing frameworks to interact with your agent. Once the token is obtained, it can be used to send messages to the agent and receive responses from the Direct Line API. [!IMPORTANT] Using “No authentication” is suitable for agents that do not access sensitive internal information. It allows anyone to interact with your agent, so use it cautiously. Select Save and close to create the agent configuration. Create a Test Set Navigate to Test Sets and click New to create a new test set for your agent. Name the test set Nova AI Test Set and Save. You can now add + New Agent Test in the subgrid. Test Set Name Type Test Utterance Expected Response TST-001 Response Match Hi! Hello, how can I help you today? TST-002 Generative Answer What caused the fall of the Roman Empire? The fall of the Roman Empire was a gradual process driven by internal issues like political instability and economic decline, and external pressures such as barbarian invasions and climate change. [!TIP] You can either add each test individually, or toggle the subgrid view to Export/Import View and select selecting ⋮ Export Agent Tests &gt; Export Agent Tests in Excel Online. This makes it very convenient to create or update multiple tests at once, especially for larger test sets. 🏅 Congratulations! You’ve completed Agent Testing Infrastructure Setup! Test your understanding Key takeaways: Authentication Configuration – Setting “No authentication” enables automated testing but should only be used for external-facing agents without sensitive data access. It is possible to confiure authentication for internal agents, but this requires additional setup. Token Endpoints – These URLs provide programmatic access to your agent and are essential for automated testing frameworks Test Set Design – Comprehensive test suites include exact matches, AI-powered analysis, and topic validation to ensure thorough coverage Lessons learned &amp; troubleshooting tips: If the token endpoint doesn’t work, ensure your agent is fully published and the authentication settings are saved Test with simple questions first before creating complex multi-turn scenarios Remember that “No authentication” agents are publicly accessible, so avoid including sensitive test data Challenge: Apply this to your own use case What types of questions do your users most commonly ask that you should include in test cases? How would you structure test scenarios to cover both happy path and error handling situations? What metrics would be most valuable for measuring your agent’s success in your specific business context? 🔄 Use Case #2: Execute and Analyze Test Results Run your configured tests and interpret the results to optimize agent performance and ensure quality. Use case Value added Estimated effort Execute and Analyze Test Results Delivers actionable insights for agent performance optimization 10 minutes Summary of tasks In this section, you’ll learn how to execute test runs, analyze detailed results including response quality and performance metrics, and use insights for continuous improvement. Scenario: Your test infrastructure is configured and you need to validate your agent’s performance across multiple scenarios. You want to establish baseline metrics and identify areas for improvement before full deployment. Step-by-step instructions Execute Test Runs Navigate to Test Runs in the Copilot Studio Kit and click New to create a new test execution. Name it Nova AI Test Run 01 Select the Agent Configuration** and Agent Test Set configured in the previous use case. Save the test run to initialize it. [!TIP] Test runs can take several minutes depending on the number of test cases and agent response times. You can monitor progress in real-time. Analyze Test Results and Performance Metrics Review the Test Run Results page which displays comprehensive analytics including: Overall pass/fail rates for your test set Individual test case results with actual vs. expected responses Response latency measurements for performance optimization Detailed AI analysis scores for generative response tests Examine failed test cases by clicking on individual results to understand why responses didn’t meet expectations. [!IMPORTANT] Pay special attention to tests that fail due to response quality rather than exact matching. These often indicate opportunities for prompt optimization or knowledge base improvements. Optimize Based on Insights Identify patterns in test failures such as: Specific topic areas where the agent struggles Response time issues that might impact user experience Inconsistent handling of similar questions Document recommended improvements based on test results, such as: Knowledge base updates for failed content questions Prompt engineering adjustments for better response quality Additional training data needs for improved accuracy [!TIP] Create a feedback loop by implementing improvements in your agent, then re-running the same test set to measure progress over time. 🏅 Congratulations! You’ve completed Test Execution and Analysis! Test your understanding How do the different test types (exact match vs. generative AI analysis) provide complementary insights into agent performance? What patterns in test results would indicate that your agent needs additional training data versus prompt optimization? How could you use test results to demonstrate ROI and continuous improvement to business stakeholders? Challenge: Apply this to your own use case Set up a regular testing schedule that aligns with your agent development and deployment cycles Create test scenarios that reflect your actual user conversations and business requirements Establish key performance indicators (KPIs) based on test results that align with your business goals 🎨 Use Case #3: Explore Advanced Kit Features Discover the comprehensive ecosystem of tools available in the Power CAT Copilot Studio Kit for enhanced agent development and management. Use case Value added Estimated effort Explore Advanced Kit Features Discovers comprehensive toolkit capabilities for enhanced agent development 10 minutes Summary of tasks In this section, you’ll explore the full range of capabilities beyond testing, including conversation analytics, SharePoint integration, UI customization, and administrative tools. Scenario: You’ve successfully implemented testing for your agents and want to understand what other capabilities are available to enhance your agent development lifecycle, improve user experience, and streamline administrative tasks. Step-by-step instructions Explore Conversation KPIs and Analytics Navigate to Conversation KPIs in the Power CAT Kit to explore advanced analytics capabilities that complement Copilot Studio’s built-in analytics. Review the dashboard features that provide aggregated conversation data in Dataverse, making it easier to understand conversation outcomes without analyzing complex transcripts. [!TIP] Conversation KPIs simplify performance tracking by providing structured data that can be easily exported and analyzed in Power BI or other reporting tools. Review Governance Tools Explore Agent Inventory which provides administrators with a tenant-wide view of all Copilot Studio agents, including usage patterns and configuration details. Investigate User Experience Tools Visit the Webchat Playground to explore visual customization options for your agent’s chat interface, including colors, fonts, and branding elements. Examine the Adaptive Cards Gallery which provides pre-built templates for rich interactive responses that enhance user engagement. [!TIP] Use the Webchat Playground to generate HTML with custom styles that match your organization’s branding guidelines. 🏅 Congratulations! You’ve explored the full Power CAT Copilot Studio Kit ecosystem! Test your understanding How do the various tools in the kit complement each other to provide a complete agent development lifecycle? Which administrative tools would be most valuable for maintaining governance and security in a large organization? How could you integrate these capabilities into your existing development and deployment processes? Challenge: Apply this to your own use case Identify which additional kit features would provide the most value for your specific agent development scenarios Plan how you could incorporate conversation KPIs and analytics into your ongoing agent optimization process Consider how administrative tools could help you scale agent development across your organization while maintaining quality and security standards 🏆 Summary of learnings True learning comes from doing, questioning, and reflecting—so let’s put your skills to the test. To maximize the impact of automated agent testing and the Power CAT ecosystem: Comprehensive Test Coverage – Include exact matches, AI-powered analysis, and edge cases to ensure thorough validation of agent behavior across all scenarios Regular Testing Cycles – Establish automated testing as part of your development workflow to catch issues early and track improvements over time Data-Driven Optimization – Use test results and conversation KPIs to make informed decisions about prompt engineering, knowledge base updates, and conversation flow improvements Performance Monitoring – Track response times and quality metrics to ensure your agent meets both functional and performance requirements Ecosystem Integration – Leverage the full range of kit capabilities including SharePoint synchronization, UI customization, and administrative tools for comprehensive agent lifecycle management Governance and Security – Use administrative tools like Agent Inventory and Agent Review Tool to maintain standards and security across your organization’s agent portfolio Conclusions and recommendations Copilot Studio Kit golden rules: Test early and test often - integrate automated testing into your development workflow from the beginning Cover both happy paths and edge cases - real users will ask unexpected questions that reveal gaps in agent training Use multiple test types for comprehensive validation - combine exact matching with AI-powered analysis for complete coverage Monitor performance trends over time - establish baselines and track improvements as you optimize your agent Implement proper governance - use the inventory tools to maintain security, compliance, and quality standards across all agents Document and share results - use test data and analytics to build confidence with stakeholders and guide business decisions Maintain test sets as living documents - update scenarios based on real user interactions and changing business needs By following these principles, you’ll deliver reliable, high-quality conversational AI experiences that consistently meet user expectations and business objectives while maintaining proper governance and security standards across your organization’s agent portfolio.","headline":"Copilot Studio Kit","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/mcs-labs/labs/copilot-studio-kit/"},"url":"http://0.0.0.0:4000/mcs-labs/labs/copilot-studio-kit/"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>
    <header class="site-header">
        <div class="container">
            <h1><a href="/mcs-labs/">Microsoft Copilot Studio Labs</a></h1>
            <nav>
                <a href="/mcs-labs/">Home</a>
                <a href="/mcs-labs/labs/">All Labs</a>
            </nav>
        </div>
    </header>

    <main class="site-content">
        <div class="container">
            <article class="lab-content">
    <header class="lab-header">
        <h1>Copilot Studio Kit</h1>
        
        <div class="lab-meta">
            <span class="duration">⏱️ 60 minutes</span>
            
            <span class="difficulty">📊 200</span>
            
        </div>
        
    </header>

    <div class="lab-body">
        <h1 id="deliver-high-quality-scalable-agents-with-copilot-studio-kit">Deliver high-quality, scalable agents with Copilot Studio Kit</h1>

<p>Automate quality assurance for your Copilot Studio agents with the Copilot Studio Kit. In this lab, you’ll configure and run automated test scenarios, validate agent behavior using multiple test types, and gain actionable insights through analytics. You’ll also explore advanced tools like conversation KPIs, UI customization, and governance dashboards—equipping you to ship high-quality, secure, and scalable AI agents with confidence.</p>

<hr />

<h2 id="-lab-details">🧭 Lab Details</h2>

<table>
  <thead>
    <tr>
      <th>Level</th>
      <th>Persona</th>
      <th>Duration</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>200</td>
      <td>Developer/Maker</td>
      <td>30 minutes</td>
      <td>After completing this lab, participants will be able to configure automated testing for Copilot Studio agents, create comprehensive test suites, and analyze agent performance through systematic validation. Gain expertise in quality assurance practices for AI-powered conversational agents.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="-table-of-contents">📚 Table of Contents</h2>

<ul>
  <li><a href="#-why-this-matters">Why This Matters</a></li>
  <li><a href="#-introduction">Introduction</a></li>
  <li><a href="#-core-concepts-overview">Core Concepts Overview</a></li>
  <li><a href="#-documentation-and-additional-training-links">Documentation and Additional Training Links</a></li>
  <li><a href="#-prerequisites">Prerequisites</a></li>
  <li><a href="#-summary-of-targets">Summary of Targets</a></li>
  <li><a href="#-use-cases-covered">Use Cases Covered</a></li>
  <li><a href="#️-instructions-by-use-case">Instructions by Use Case</a>
    <ul>
      <li><a href="#-use-case-1-configure-agent-testing-infrastructure">Use Case #1: Configure Agent Testis</a></li>
      <li><a href="#-use-case-2-execute-and-analyze-test-results">Use Case #2: Execute and Analyze Test Results</a></li>
      <li><a href="#-use-case-3-explore-the-other-copilot-studio-kit-features">Use Case #3: Explore the other Copilot Studio Kit features</a></li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="-why-this-matters">🤔 Why This Matters</h2>

<p><strong>Makers and Developers</strong> - Worried about deploying AI agents that might give inconsistent or incorrect responses to users?</p>

<p>Think of testing a chatbot like quality control in manufacturing:</p>
<ul>
  <li><strong>Without systematic testing</strong>: Agents deployed with unknown behavior, potential for embarrassing mistakes, and no confidence in responses</li>
  <li><strong>With automated testing</strong>: Validated agent performance, consistent quality assurance, and data-driven optimization insights</li>
</ul>

<p><strong>Common challenges solved by this lab:</strong></p>
<ul>
  <li>“How do I know if my agent is giving good answers?”</li>
  <li>“I need to test multiple scenarios without manually chatting hundreds of times”</li>
  <li>“My agent works sometimes but fails on edge cases I didn’t think of”</li>
  <li>“I want to track performance improvements over time”</li>
</ul>

<p><strong>Spend 30 minutes now to save hours of manual testing and avoid costly production issues later.</strong></p>

<hr />

<h2 id="-introduction">🌐 Introduction</h2>

<p>Quality assurance for AI agents requires a systematic approach that goes beyond manual testing. The Power CAT Copilot Studio Kit provides enterprise-grade testing capabilities that enable makers to validate agent behavior at scale.</p>

<p><strong>Real-world example:</strong> A customer service organization deploys a Copilot Studio agent to handle common inquiries about product returns. Without proper testing, the agent might incorrectly handle edge cases like partial returns or international shipping, leading to frustrated customers and escalated support tickets. With automated testing, they can validate hundreds of scenarios before deployment and catch issues early.</p>

<p>This lab teaches you to implement professional testing practices that ensure your agents deliver consistent, reliable experiences to end users.</p>

<hr />

<h2 id="-core-concepts-overview">🎓 Core Concepts Overview</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>Why it matters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Agent Configuration</strong></td>
      <td>Defines the connection between your Copilot Studio agent and the testing framework, enabling automated interactions and response validation</td>
    </tr>
    <tr>
      <td><strong>Test Sets and Test Cases</strong></td>
      <td>Structured collections of scenarios that systematically validate agent behavior across different conversation patterns and edge cases</td>
    </tr>
    <tr>
      <td><strong>Test Types</strong></td>
      <td>Different validation approaches (response &amp; topic match, generative AI analysis, adaptive cards) that ensure comprehensive coverage of agent capabilities</td>
    </tr>
    <tr>
      <td><strong>Test Runs and Analytics</strong></td>
      <td>Execution framework that provides detailed performance metrics, response analysis, and trending data for continuous improvement</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="-documentation-and-additional-training-links">📄 Documentation and Additional Training Links</h2>

<ul>
  <li><a href="https://aka.ms/CopilotStudioKit">Power CAT Copilot Studio Kit Documentation</a></li>
</ul>

<hr />

<h2 id="-prerequisites">✅ Prerequisites</h2>

<ul>
  <li>Access to a Microsoft Copilot Studio environment with agent creation permissions.</li>
  <li>Access to the Power CAT Copilot Studio Kit (installed from AppSource or GitHub).</li>
</ul>

<hr />

<h2 id="-summary-of-targets">🎯 Summary of Targets</h2>

<p>In this lab, you’ll establish a comprehensive testing framework for Copilot Studio agents. By the end of the lab, you will:</p>

<ul>
  <li>Set up agent configuration in the Copilot Studio Kit.</li>
  <li>Create structured test sets with multiple validation scenarios including exact matches and AI-powered response analysis.</li>
  <li>Execute automated test runs and interpret detailed insights.</li>
  <li>Explore other features of the kit.</li>
</ul>

<hr />

<h2 id="-use-cases-covered">🧩 Use Cases Covered</h2>

<table>
  <thead>
    <tr>
      <th>Step</th>
      <th>Use Case</th>
      <th>Value added</th>
      <th>Effort</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td><a href="#-use-case-1-configure-agent-tests">Configure Agent Tests</a></td>
      <td>Establishes automated testing pipeline for systematic agent validation</td>
      <td>10 min</td>
    </tr>
    <tr>
      <td>2</td>
      <td><a href="#-use-case-2-execute-and-analyze-test-results">Execute and Analyze Test Results</a></td>
      <td>Delivers actionable insights for agent performance optimization</td>
      <td>10 min</td>
    </tr>
    <tr>
      <td>3</td>
      <td><a href="#-use-case-3-explore-the-other-copilot-studio-kit-features">Explore the other Copilot Studio Kit features</a></td>
      <td>Beyond testing, the Power CAT Copilot Studio Kit includes powerful tools for real-world deployment and governance: conversation KPIs for tracking agent performance, SharePoint synchronization for auto-updating knowledge bases, the Webchat Playground for UI customization, and the Adaptive Cards Gallery for rich interactions. It also offers admin tools like Agent Inventory for environment-wide visibility and the Agent Review Tool to detect issues and anti-patterns—helping makers ship secure, scalable, and effective agents.</td>
      <td>10 min</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="️-instructions-by-use-case">🛠️ Instructions by Use Case</h2>

<hr />

<h2 id="-use-case-1-configure-agent-tests">🧱 Use Case #1: Configure Agent Tests</h2>

<p>Set up the foundational components needed to automate testing of your Copilot Studio agents.</p>

<table>
  <thead>
    <tr>
      <th>Use case</th>
      <th>Value added</th>
      <th>Estimated effort</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Configure Agent Testing Infrastructure</td>
      <td>Establishes automated testing pipeline for systematic agent validation</td>
      <td>20 minutes</td>
    </tr>
  </tbody>
</table>

<p><strong>Summary of tasks</strong></p>

<p>In this section, you’ll learn how to configure an external-facing agent for testing, connect it to the Power CAT Copilot Studio Kit, and create comprehensive test scenarios.</p>

<p><strong>Scenario:</strong> You’re a maker responsible for a customer support agent that handles product inquiries. Before deploying to production, you need to ensure the agent responds appropriately to various customer questions and edge cases.</p>

<h3 id="objective">Objective</h3>

<p>Configure a Copilot Studio agent for external access and integrate it with the Power CAT testing framework to enable automated validation.</p>

<hr />

<h3 id="step-by-step-instructions">Step-by-step instructions</h3>

<h4 id="access-the-copilot-studio-kit-application">Access the Copilot Studio Kit application</h4>

<ol>
  <li><strong>Start</strong> by accessing the Copilot Studio Kit application, either installed in your organization or the one that was setup for the training.</li>
</ol>

<blockquote>
  <p>[!IMPORTANT]</p>
  <ul>
    <li>To access this app, use the provided URL in the <strong>Lab Resources</strong> (specific per training).</li>
  </ul>
</blockquote>

<ol>
  <li>The first time you open the app, you will be prompted to allow Copilot Studio Kit to access your data. Select <strong>Allow</strong> to grant permissions.</li>
</ol>

<h4 id="create-an-agent-configuration">Create an agent configuration</h4>

<ol>
  <li>
    <p>Go to <strong>Agents</strong> in the left navigation menu.</p>
  </li>
  <li>
    <p>Select <strong>New</strong></p>
  </li>
  <li>
    <p><strong>Name</strong> it <code class="language-plaintext highlighter-rouge">Nova AI</code> and select <strong>Test Automation</strong> as <strong>Configuration Type</strong>.</p>
  </li>
  <li>
    <p>To obtain the <strong>Token Endpoint</strong>, you need to navigate to your <strong>Nova AI</strong> agent, go to <strong>Channels</strong>, select <strong>Direct Line Speech</strong>, and copy the <strong>Token Endpoint</strong> URL. Paste this URL into the <strong>Token Endpoint</strong> field in the Copilot Studio Kit.</p>
  </li>
</ol>

<blockquote>
  <p>[!TIP]
The <strong>tokent endpoint</strong> allows to obtain a token that can be used to initiate a conversation with the agent programmatically. This is essential for automated testing frameworks to interact with your agent. Once the token is obtained, it can be used to send messages to the agent and receive responses from the <strong>Direct Line API</strong>.</p>
</blockquote>

<blockquote>
  <p>[!IMPORTANT]
Using “No authentication” is suitable for agents that do not access sensitive internal information. It allows anyone to interact with your agent, so use it cautiously.</p>
</blockquote>

<ol>
  <li>Select <strong>Save and close</strong> to create the agent configuration.</li>
</ol>

<h4 id="create-a-test-set">Create a Test Set</h4>

<ol>
  <li>
    <p><strong>Navigate</strong> to <strong>Test Sets</strong> and click <strong>New</strong> to create a new test set for your agent.</p>
  </li>
  <li>
    <p><strong>Name</strong> the test set <code class="language-plaintext highlighter-rouge">Nova AI Test Set</code> and <strong>Save</strong>.</p>
  </li>
  <li>
    <p>You can now add <strong>+ New Agent Test</strong> in the subgrid.</p>
  </li>
</ol>

<table>
  <thead>
    <tr>
      <th>Test Set Name</th>
      <th>Type</th>
      <th>Test Utterance</th>
      <th>Expected Response</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">TST-001</code></td>
      <td><strong>Response Match</strong></td>
      <td><code class="language-plaintext highlighter-rouge">Hi!</code></td>
      <td><code class="language-plaintext highlighter-rouge">Hello, how can I help you today?</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">TST-002</code></td>
      <td><strong>Generative Answer</strong></td>
      <td><code class="language-plaintext highlighter-rouge">What caused the fall of the Roman Empire?</code></td>
      <td><code class="language-plaintext highlighter-rouge">The fall of the Roman Empire was a gradual process driven by internal issues like political instability and economic decline, and external pressures such as barbarian invasions and climate change.</code></td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>[!TIP]
You can either add each test individually, or toggle the subgrid view to <strong>Export/Import View</strong> and select selecting <code class="language-plaintext highlighter-rouge">⋮</code> <strong>Export Agent Tests</strong> &gt; <strong>Export Agent Tests in Excel Online</strong>. This makes it very convenient to create or update multiple tests at once, especially for larger test sets.</p>
</blockquote>

<hr />

<h3 id="-congratulations-youve-completed-agent-testing-infrastructure-setup">🏅 Congratulations! You’ve completed Agent Testing Infrastructure Setup!</h3>

<hr />

<h3 id="test-your-understanding">Test your understanding</h3>

<p><strong>Key takeaways:</strong></p>

<ul>
  <li><strong>Authentication Configuration</strong> – Setting “No authentication” enables automated testing but should only be used for external-facing agents without sensitive data access. It is possible to confiure authentication for internal agents, but this requires additional setup.</li>
  <li><strong>Token Endpoints</strong> – These URLs provide programmatic access to your agent and are essential for automated testing frameworks</li>
  <li><strong>Test Set Design</strong> – Comprehensive test suites include exact matches, AI-powered analysis, and topic validation to ensure thorough coverage</li>
</ul>

<p><strong>Lessons learned &amp; troubleshooting tips:</strong></p>

<ul>
  <li>If the token endpoint doesn’t work, ensure your agent is fully published and the authentication settings are saved</li>
  <li>Test with simple questions first before creating complex multi-turn scenarios</li>
  <li>Remember that “No authentication” agents are publicly accessible, so avoid including sensitive test data</li>
</ul>

<p><strong>Challenge: Apply this to your own use case</strong></p>

<ul>
  <li>What types of questions do your users most commonly ask that you should include in test cases?</li>
  <li>How would you structure test scenarios to cover both happy path and error handling situations?</li>
  <li>What metrics would be most valuable for measuring your agent’s success in your specific business context?</li>
</ul>

<hr />

<hr />

<h2 id="-use-case-2-execute-and-analyze-test-results">🔄 Use Case #2: Execute and Analyze Test Results</h2>

<p>Run your configured tests and interpret the results to optimize agent performance and ensure quality.</p>

<table>
  <thead>
    <tr>
      <th>Use case</th>
      <th>Value added</th>
      <th>Estimated effort</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Execute and Analyze Test Results</td>
      <td>Delivers actionable insights for agent performance optimization</td>
      <td>10 minutes</td>
    </tr>
  </tbody>
</table>

<p><strong>Summary of tasks</strong></p>

<p>In this section, you’ll learn how to execute test runs, analyze detailed results including response quality and performance metrics, and use insights for continuous improvement.</p>

<p><strong>Scenario:</strong> Your test infrastructure is configured and you need to validate your agent’s performance across multiple scenarios. You want to establish baseline metrics and identify areas for improvement before full deployment.</p>

<h3 id="step-by-step-instructions-1">Step-by-step instructions</h3>

<h4 id="execute-test-runs">Execute Test Runs</h4>

<ol>
  <li>
    <p><strong>Navigate</strong> to <strong>Test Runs</strong> in the Copilot Studio Kit and click <strong>New</strong> to create a new test execution.</p>
  </li>
  <li>
    <p><strong>Name</strong> it <code class="language-plaintext highlighter-rouge">Nova AI Test Run 01</code></p>
  </li>
  <li>
    <p><strong>Select</strong> the <strong>Agent Configuratio</strong>n** and <strong>Agent Test Set</strong> configured in the previous use case.</p>
  </li>
  <li>
    <p><strong>Save</strong> the test run to initialize it.</p>
  </li>
</ol>

<blockquote>
  <p>[!TIP]
Test runs can take several minutes depending on the number of test cases and agent response times. You can monitor progress in real-time.</p>
</blockquote>

<h4 id="analyze-test-results-and-performance-metrics">Analyze Test Results and Performance Metrics</h4>

<ol>
  <li><strong>Review the Test Run Results</strong> page which displays comprehensive analytics including:
    <ul>
      <li>Overall pass/fail rates for your test set</li>
      <li>Individual test case results with actual vs. expected responses</li>
      <li>Response latency measurements for performance optimization</li>
      <li>Detailed AI analysis scores for generative response tests</li>
    </ul>
  </li>
  <li><strong>Examine failed test cases</strong> by clicking on individual results to understand why responses didn’t meet expectations.</li>
</ol>

<blockquote>
  <p>[!IMPORTANT]
Pay special attention to tests that fail due to response quality rather than exact matching. These often indicate opportunities for prompt optimization or knowledge base improvements.</p>
</blockquote>

<h4 id="optimize-based-on-insights">Optimize Based on Insights</h4>

<ol>
  <li><strong>Identify patterns in test failures</strong> such as:
    <ul>
      <li>Specific topic areas where the agent struggles</li>
      <li>Response time issues that might impact user experience</li>
      <li>Inconsistent handling of similar questions</li>
    </ul>
  </li>
  <li><strong>Document recommended improvements</strong> based on test results, such as:
    <ul>
      <li>Knowledge base updates for failed content questions</li>
      <li>Prompt engineering adjustments for better response quality</li>
      <li>Additional training data needs for improved accuracy</li>
    </ul>
  </li>
</ol>

<blockquote>
  <p>[!TIP]
Create a feedback loop by implementing improvements in your agent, then re-running the same test set to measure progress over time.</p>
</blockquote>

<hr />

<h3 id="-congratulations-youve-completed-test-execution-and-analysis">🏅 Congratulations! You’ve completed Test Execution and Analysis!</h3>

<hr />

<h3 id="test-your-understanding-1">Test your understanding</h3>

<ul>
  <li>How do the different test types (exact match vs. generative AI analysis) provide complementary insights into agent performance?</li>
  <li>What patterns in test results would indicate that your agent needs additional training data versus prompt optimization?</li>
  <li>How could you use test results to demonstrate ROI and continuous improvement to business stakeholders?</li>
</ul>

<p><strong>Challenge: Apply this to your own use case</strong></p>

<ul>
  <li>Set up a regular testing schedule that aligns with your agent development and deployment cycles</li>
  <li>Create test scenarios that reflect your actual user conversations and business requirements</li>
  <li>Establish key performance indicators (KPIs) based on test results that align with your business goals</li>
</ul>

<hr />

<hr />

<h2 id="-use-case-3-explore-advanced-kit-features">🎨 Use Case #3: Explore Advanced Kit Features</h2>

<p>Discover the comprehensive ecosystem of tools available in the Power CAT Copilot Studio Kit for enhanced agent development and management.</p>

<table>
  <thead>
    <tr>
      <th>Use case</th>
      <th>Value added</th>
      <th>Estimated effort</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Explore Advanced Kit Features</td>
      <td>Discovers comprehensive toolkit capabilities for enhanced agent development</td>
      <td>10 minutes</td>
    </tr>
  </tbody>
</table>

<p><strong>Summary of tasks</strong></p>

<p>In this section, you’ll explore the full range of capabilities beyond testing, including conversation analytics, SharePoint integration, UI customization, and administrative tools.</p>

<p><strong>Scenario:</strong> You’ve successfully implemented testing for your agents and want to understand what other capabilities are available to enhance your agent development lifecycle, improve user experience, and streamline administrative tasks.</p>

<h3 id="step-by-step-instructions-2">Step-by-step instructions</h3>

<h4 id="explore-conversation-kpis-and-analytics">Explore Conversation KPIs and Analytics</h4>

<ol>
  <li>
    <p><strong>Navigate to Conversation KPIs</strong> in the Power CAT Kit to explore advanced analytics capabilities that complement Copilot Studio’s built-in analytics.</p>
  </li>
  <li>
    <p><strong>Review the dashboard features</strong> that provide aggregated conversation data in Dataverse, making it easier to understand conversation outcomes without analyzing complex transcripts.</p>

    <p><img src="images/conversation-kpis.png" alt="alt text" /></p>

    <p><img src="images/conversation-transcript-viewer.png" alt="alt text" /></p>
  </li>
</ol>

<blockquote>
  <p>[!TIP]
Conversation KPIs simplify performance tracking by providing structured data that can be easily exported and analyzed in Power BI or other reporting tools.</p>
</blockquote>

<h4 id="review-governance-tools">Review Governance Tools</h4>

<ol>
  <li>
    <p><strong>Explore Agent Inventory</strong> which provides administrators with a tenant-wide view of all Copilot Studio agents, including usage patterns and configuration details.</p>

    <p><img src="images/agent-inventory.png" alt="alt text" /></p>
  </li>
</ol>

<h4 id="investigate-user-experience-tools">Investigate User Experience Tools</h4>

<ol>
  <li>
    <p><strong>Visit the Webchat Playground</strong> to explore visual customization options for your agent’s chat interface, including colors, fonts, and branding elements.</p>

    <p><img src="images/web-chat-playground.png" alt="alt text" /></p>
  </li>
  <li>
    <p><strong>Examine the Adaptive Cards Gallery</strong> which provides pre-built templates for rich interactive responses that enhance user engagement.</p>
  </li>
</ol>

<p><img src="images/adaptive-cards-gallery.png" alt="alt text" /></p>

<blockquote>
  <p>[!TIP]
Use the Webchat Playground to generate HTML with custom styles that match your organization’s branding guidelines.</p>
</blockquote>

<hr />

<h3 id="-congratulations-youve-explored-the-full-power-cat-copilot-studio-kit-ecosystem">🏅 Congratulations! You’ve explored the full Power CAT Copilot Studio Kit ecosystem!</h3>

<hr />

<h3 id="test-your-understanding-2">Test your understanding</h3>

<ul>
  <li>How do the various tools in the kit complement each other to provide a complete agent development lifecycle?</li>
  <li>Which administrative tools would be most valuable for maintaining governance and security in a large organization?</li>
  <li>How could you integrate these capabilities into your existing development and deployment processes?</li>
</ul>

<p><strong>Challenge: Apply this to your own use case</strong></p>

<ul>
  <li>Identify which additional kit features would provide the most value for your specific agent development scenarios</li>
  <li>Plan how you could incorporate conversation KPIs and analytics into your ongoing agent optimization process</li>
  <li>Consider how administrative tools could help you scale agent development across your organization while maintaining quality and security standards</li>
</ul>

<h2 id="-summary-of-learnings">🏆 Summary of learnings</h2>

<p>True learning comes from doing, questioning, and reflecting—so let’s put your skills to the test.</p>

<p>To maximize the impact of automated agent testing and the Power CAT ecosystem:</p>

<ul>
  <li><strong>Comprehensive Test Coverage</strong> – Include exact matches, AI-powered analysis, and edge cases to ensure thorough validation of agent behavior across all scenarios</li>
  <li><strong>Regular Testing Cycles</strong> – Establish automated testing as part of your development workflow to catch issues early and track improvements over time</li>
  <li><strong>Data-Driven Optimization</strong> – Use test results and conversation KPIs to make informed decisions about prompt engineering, knowledge base updates, and conversation flow improvements</li>
  <li><strong>Performance Monitoring</strong> – Track response times and quality metrics to ensure your agent meets both functional and performance requirements</li>
  <li><strong>Ecosystem Integration</strong> – Leverage the full range of kit capabilities including SharePoint synchronization, UI customization, and administrative tools for comprehensive agent lifecycle management</li>
  <li><strong>Governance and Security</strong> – Use administrative tools like Agent Inventory and Agent Review Tool to maintain standards and security across your organization’s agent portfolio</li>
</ul>

<hr />

<h3 id="conclusions-and-recommendations">Conclusions and recommendations</h3>

<p><strong>Copilot Studio Kit golden rules:</strong></p>

<ul>
  <li>Test early and test often - integrate automated testing into your development workflow from the beginning</li>
  <li>Cover both happy paths and edge cases - real users will ask unexpected questions that reveal gaps in agent training</li>
  <li>Use multiple test types for comprehensive validation - combine exact matching with AI-powered analysis for complete coverage</li>
  <li>Monitor performance trends over time - establish baselines and track improvements as you optimize your agent</li>
  <li>Implement proper governance - use the inventory tools to maintain security, compliance, and quality standards across all agents</li>
  <li>Document and share results - use test data and analytics to build confidence with stakeholders and guide business decisions</li>
  <li>Maintain test sets as living documents - update scenarios based on real user interactions and changing business needs</li>
</ul>

<p>By following these principles, you’ll deliver reliable, high-quality conversational AI experiences that consistently meet user expectations and business objectives while maintaining proper governance and security standards across your organization’s agent portfolio.</p>

<hr />

    </div>

    <footer class="lab-footer">
        <div class="lab-navigation">
            
            
            
            
            
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                
                
            
                
                    
                    
            
            
                <a href="/mcs-labs/labs/pipelines-and-source-control/" class="nav-button prev">
                    ← Pipelines and Source Control
                </a>
            
            
            
                <a href="/mcs-labs/labs/measure-success/" class="nav-button next">
                    Measure Success →
                </a>
            
        </div>
    </footer>
</article>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p>&copy; 2025 Microsoft Copilot Studio Labs</p>
        </div>
    </footer>
</body>
</html>